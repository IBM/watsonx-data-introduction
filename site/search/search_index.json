{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introducing watsonx.data Watsonx.data is a core component of watsonx, IBM\u2019s enterprise-ready AI and data platform designed to multiply the impact of AI across an enterprise\u2019s business. The watsonx platform comprises three powerful components: the watsonx.ai studio for new foundation models, generative AI, and machine learning; the watsonx.data fit-for-purpose data store that provides the flexibility of a data lake with the performance of a data warehouse; plus, the watsonx.governance toolkit, to enable AI workflows that are built with responsibility, transparency, and explainability. The watsonx.data component (the focus of this lab) makes it possible for enterprises to scale analytics and AI with a data store built on an open lakehouse architecture, supported by querying, governance, and open data and table formats, to access and share data. With watsonx.data, enterprises can connect to data in minutes, quickly get trusted insights, and reduce their data warehouse costs. The next-gen watsonx.data lakehouse is designed to overcome the costs and complexities enterprises face. This will be the world\u2019s first and only open data store with multi-engine support that is built for hybrid deployment across your entire ecosystem. Watsonx.data is the only lakehouse with multiple query engines allowing you to optimize costs and performance by pairing the right workload with the right engine. Run all workloads from a single pane of glass, eliminating trade-offs with convenience while still improving cost and performance. Deploy anywhere with full support for hybrid-cloud and multi cloud environments. Shared metadata across multiple engines eliminates the need to re-catalog, accelerating time to value while ensuring governance and eliminating costly implementation efforts. Watsonx.data Lab The watsonx.data hands-on lab introduces you to several core components and capabilities of IBM watsonx.data. By completing this lab, you will gain and understanding of what the watsonx.data platform provides for users. Specifically, you will get hands-on experience in the following areas: The watsonx.data web-based user interface (UI), including infrastructure management, data management, running SQL statements, and managing user access An introduction to Presto SQL Running queries that combine data from multiple data sources (data federation) Offloading tables from Db2 into watsonx.data Rolling back a table to a previous point in time This lab requires that a workshop environment be provisioned for you using the IBM Technology Zone (TechZone). The image used comes pre-configured with watsonx.data Developer Edition, additional database systems including Db2 and PostgreSQL, and sample data sets. Watsonx.data Dialogs and Screens Watsonx.data is being developed and released in an agile manner. In addition to new capabilities being added, the web interface is also likely to change over time. Therefore, the screenshots used in this lab may not always look exactly like what you see.","title":"Introduction"},{"location":"#introducing-watsonxdata","text":"Watsonx.data is a core component of watsonx, IBM\u2019s enterprise-ready AI and data platform designed to multiply the impact of AI across an enterprise\u2019s business. The watsonx platform comprises three powerful components: the watsonx.ai studio for new foundation models, generative AI, and machine learning; the watsonx.data fit-for-purpose data store that provides the flexibility of a data lake with the performance of a data warehouse; plus, the watsonx.governance toolkit, to enable AI workflows that are built with responsibility, transparency, and explainability. The watsonx.data component (the focus of this lab) makes it possible for enterprises to scale analytics and AI with a data store built on an open lakehouse architecture, supported by querying, governance, and open data and table formats, to access and share data. With watsonx.data, enterprises can connect to data in minutes, quickly get trusted insights, and reduce their data warehouse costs. The next-gen watsonx.data lakehouse is designed to overcome the costs and complexities enterprises face. This will be the world\u2019s first and only open data store with multi-engine support that is built for hybrid deployment across your entire ecosystem. Watsonx.data is the only lakehouse with multiple query engines allowing you to optimize costs and performance by pairing the right workload with the right engine. Run all workloads from a single pane of glass, eliminating trade-offs with convenience while still improving cost and performance. Deploy anywhere with full support for hybrid-cloud and multi cloud environments. Shared metadata across multiple engines eliminates the need to re-catalog, accelerating time to value while ensuring governance and eliminating costly implementation efforts.","title":"Introducing watsonx.data"},{"location":"#watsonxdata-lab","text":"The watsonx.data hands-on lab introduces you to several core components and capabilities of IBM watsonx.data. By completing this lab, you will gain and understanding of what the watsonx.data platform provides for users. Specifically, you will get hands-on experience in the following areas: The watsonx.data web-based user interface (UI), including infrastructure management, data management, running SQL statements, and managing user access An introduction to Presto SQL Running queries that combine data from multiple data sources (data federation) Offloading tables from Db2 into watsonx.data Rolling back a table to a previous point in time This lab requires that a workshop environment be provisioned for you using the IBM Technology Zone (TechZone). The image used comes pre-configured with watsonx.data Developer Edition, additional database systems including Db2 and PostgreSQL, and sample data sets. Watsonx.data Dialogs and Screens Watsonx.data is being developed and released in an agile manner. In addition to new capabilities being added, the web interface is also likely to change over time. Therefore, the screenshots used in this lab may not always look exactly like what you see.","title":"Watsonx.data Lab"},{"location":"wxd-accesscontrol/","text":"Access Control Security and access control within watsonx.data are based on roles. A role is a set of privileges that control the actions that users can perform. Authorization is granted by assigning a specific role to a user, or by adding the user to a group that has been assigned one or more roles. Access control at the infrastructural level allows permissions to be granted on the engines, catalogs, buckets, and databases. Roles for these components include Admin, Manager, User, Writer, and Reader (depending on the component). Access to the data itself is managed through data control policies. Policies can be created to permit or deny access to schemas, tables, and columns. User account management and access management varies between the different deployment options for watsonx.data. For instance, in the managed cloud service (SaaS), the service owner would need to invite other users to the environment and give them appropriate service access. With the standalone software, users can be added within the console\u2019s Access control page. In the Developer Edition, users can be added using a command line tool. Click on the Access Control icon on the left side of the screen The Access Control panel is divided into Infrastructure and Policies. The initial screen displays the objects that are considered part of the Infrastructure. A user must be granted access to the infrastructure (i.e. Presto engine) in order for them to run queries against any of the schemas in the database. Click on the Policies Tab Your system should not contain any policies at this time. Roles and Policies In this section you will add a new user and provide them with privileges over the infrastructure and data. First start by adding a new user to the watsonx.data system. This lab is using the Developer edition of the watsonx.data software, which means that the Access control panel does not provide a UI for adding users. In order to manage users, the user-mgmt command will need to be used. The user-mgmt command is normally run from a command line in the server. ./user-mgmt add-user [User | Admin] The values are: username - The name of the user [User|Admin] - The type of user. Note that the type of user is case-sensitive! password - The password for the user. For the purposes of this lab, we are going to use a Jupyter notebook that was specifically created for managing users. Find the URL in your reservation that reads Jupyter Notebook - Server: http://useast.services.cloud.techzone.ibm.com:xxxx/notebooks/Table_of_Contents.ipynb and click on it The initial screen will request that you enter a password. The password is watsonx.data . Enter watsonx.data as the password Once you have authenticated, the main table of contents will be displayed. You are going to use the User Administration notebook. Click on the blue arrow in the User Administration tile The browser will display the User Administration notebook. A Jupyter notebook contains multiple \"cells\" which can contain one of three different types of objects: Code - A cell that contains code that will run (usually Python) Markdown - A cell that contains text and formatting using a language called Markdown Raw NBConvert - A specialized cell that is rendered (displayed) using an extension, like mathematical formulas In this lab, we need to run code that is found in the notebook. You can tell which text is a code cell because of the [ ]: beside the cell and probably because it has some code in the cell! Before executing the contents of the cell, you must click on the cell (place focus on the cell). You can tell when the focus is on the code cell because it will be highlighted with a thin blue box. To \"execute\" the contents of the cell, you must either hit the run button icon \u25ba or Shift-Return on your keyboard. Run the contents of the cell which contains the add-user command A message indicating that a user was successfully added will be displayed in the notebook. ['Adding password for user watsonx'] At this point you will return to the browser page which contains the watsonx.data UI. Click on the browser tab which contains the watsonx.data UI Access Control To view what users are currently authorized to use the system, select the Access control icon found on the left side of the watsonx.data UI. Click on the Access Control icon on the left side of the screen A list of objects that make up the watsonx.data infrastructure are displayed. You can see that the objects are made up of: Engines Catalogs Buckets In a real-world scenario where a user will be querying data from a table, that user will need to be given a minimum of User access to an engine (to be able to run the query), User access for the catalog associated with the data (to be able to see the schema information associated with the table), and Reader access to the bucket associated with the data (to be able to read the data from object storage). Additionally, a policy has to be created to permit the user to access the table in question. Granting Access Select the presto-01 engine (highlighted in red) to view the current users that have access to the engine At this point, only the administrative user (ibmlhadmin) can use the Presto engine. Click on the Add Access button to add a new authorized user to the list If you do not see the watsonx userid, or the dropdown is blank, refresh the browser screen The role button has been selected in the dialog to show the role options of Admin, Manager, or User. An Admin user can grant any role to a user, while a Manager can only grant User privileges. Grant watsonx \"user\" privileges and then press Add Close the dialog by clicking on the [x] on the top right of the screen The watsonx user needs to be granted access to a catalog. In this case, the iceberg_data and hive_data catalogs are required for the Presto engine and are implicitly granted to the user. Click on the iceberg_data line to confirm that watsonx has access You should see that watsonx has already been granted access to the catalog. Close the dialog to return to the Access control screen The final step is to grant access to the underlying buckets. Because watsonx was implicitly granted access to the iceberg_data and hive_data catalogs, the underlying buckets iceberg-bucket and hive-bucket were also added to their access list. Click on iceberg-bucket to view the access control Close the dialog to return to the Access control dialog Policies After access has been granted to engines, catalogs, and buckets, the final step is to create a policy to grant access to tables. Click on the Policy tab to display the current policies in place (there should be none) The Add Policy button is highlighted on the far right side of the screen. Press the Add Policy button to display the new Access Control Policy dialog Fill in the following fields with these values Policy name: selectflights Description: blank Policy status: active You can always activate a policy after you have created it. Click Next Here we need to select which catalog to use and then the schemas that the user will be able to access. Select hive_data catalog, and then the ontime schema After selecting the schema, a set of tables associated with the schema will be listed. You can choose which tables can be searched. If you choose an individual table, you can restrict which columns can be searched. Select the ontime table and then select the following columns (you will need to scroll down the page) flightdate reporting_airline flight_number_reporting_airline originairportid destairportid Press the Next button when all columns are selected The Rules dialog allows you to fine-tune what actions can be done by a user against the data. Press the Add Rule button to display the Add Rule dialog Rules can be used to Allow actions or to Deny actions. Allow watsonx to SELECT data from the data, but with no other options Note : In production versions of watsonx.data, you can provide access to a group which makes it simpler to create a set of rules that apply to a group and then add a user to a group. That way a user will inherit the rules that were applied to the group rather than having to create rules for that individual. The developer edition displays GROUP as an option, but it is not implemented. Press the Add button You can continue to add additional rules to the list. Since we only want the one rule, select the Review button. Press the Review button Confirm that the data objects and rules look correct then press the Save button The selectflights policy is now in place and is actively enforced. Press the Query Workspace Icon Before testing the policy enforcement, use the SQL icon on the left side to navigate to the hive_data catalog and view the schemas that are associated with it. Expand the ontime schema to view the tables and the columns that are available in the ontime table When you connect as watsonx , you will be able to compare what objects can be accessed from that userid. Testing Policy Enforcement To test whether the rules are enforced, you will need to log out of the current watsonx.data UI session. At the top of the Query workspace screen, you will see the user icon on the top right. Click on the user icon to display the logout dialog Log out to the main watsonx.data UI screen You will be prompted to confirm log out. The log in dialog should be displayed. Enter the credentials for the watsonx user and press Log in Username: watsonx Password: watsonx.data Your watsonx.data UI should now display watsonx . Navigate to the SQL icon and then select hive_data \u2192 ontime \u2192 ontime You should notice that watsonx was restricted to seeing only the ontime schema in the hive_data catalog. In addition, the user was restricted to accessing one of the tables ( ontime ) and 5 columns. Attempting to run a SELECT statement against all the data will result in a policy error. Enter this text into the SQL window and Run the code SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; Correcting the SQL to include only permitted columns results in an answer set. Enter this text into the SQL window and Run the code SELECT flightdate, reporting_airline, flight_number_reporting_airline, originairportid, destairportid FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; The policy rules have been enforced for watsonx , preventing them from viewing any other schemas or tables in the system. In addition, the SQL that they could execute was restricted to specific columns in the table. Before moving onto any other sections, make sure to log out as watsonx and reconnect as the ibmlhadmin user. Click on the user icon to display the logout dialog Log out to the main watsonx.data UI screen You will be prompted to confirm log out. The log in dialog should be displayed. Enter the credentials for the ibmlhadmin user and press Log in Username: ibmlhadmin Password: password Your watsonx.data UI should now display ibmlhadmin . Summary In this section you learned how to add a user to watsonx.data (via a command) and control what objects they are allowed to access. In production versions of watsonx.data, the adding and removal of users is directly handled through the User Interface.","title":"Access Control"},{"location":"wxd-accesscontrol/#access-control","text":"Security and access control within watsonx.data are based on roles. A role is a set of privileges that control the actions that users can perform. Authorization is granted by assigning a specific role to a user, or by adding the user to a group that has been assigned one or more roles. Access control at the infrastructural level allows permissions to be granted on the engines, catalogs, buckets, and databases. Roles for these components include Admin, Manager, User, Writer, and Reader (depending on the component). Access to the data itself is managed through data control policies. Policies can be created to permit or deny access to schemas, tables, and columns. User account management and access management varies between the different deployment options for watsonx.data. For instance, in the managed cloud service (SaaS), the service owner would need to invite other users to the environment and give them appropriate service access. With the standalone software, users can be added within the console\u2019s Access control page. In the Developer Edition, users can be added using a command line tool. Click on the Access Control icon on the left side of the screen The Access Control panel is divided into Infrastructure and Policies. The initial screen displays the objects that are considered part of the Infrastructure. A user must be granted access to the infrastructure (i.e. Presto engine) in order for them to run queries against any of the schemas in the database. Click on the Policies Tab Your system should not contain any policies at this time.","title":"Access Control"},{"location":"wxd-accesscontrol/#roles-and-policies","text":"In this section you will add a new user and provide them with privileges over the infrastructure and data. First start by adding a new user to the watsonx.data system. This lab is using the Developer edition of the watsonx.data software, which means that the Access control panel does not provide a UI for adding users. In order to manage users, the user-mgmt command will need to be used. The user-mgmt command is normally run from a command line in the server. ./user-mgmt add-user [User | Admin] The values are: username - The name of the user [User|Admin] - The type of user. Note that the type of user is case-sensitive! password - The password for the user. For the purposes of this lab, we are going to use a Jupyter notebook that was specifically created for managing users. Find the URL in your reservation that reads Jupyter Notebook - Server: http://useast.services.cloud.techzone.ibm.com:xxxx/notebooks/Table_of_Contents.ipynb and click on it The initial screen will request that you enter a password. The password is watsonx.data . Enter watsonx.data as the password Once you have authenticated, the main table of contents will be displayed. You are going to use the User Administration notebook. Click on the blue arrow in the User Administration tile The browser will display the User Administration notebook. A Jupyter notebook contains multiple \"cells\" which can contain one of three different types of objects: Code - A cell that contains code that will run (usually Python) Markdown - A cell that contains text and formatting using a language called Markdown Raw NBConvert - A specialized cell that is rendered (displayed) using an extension, like mathematical formulas In this lab, we need to run code that is found in the notebook. You can tell which text is a code cell because of the [ ]: beside the cell and probably because it has some code in the cell! Before executing the contents of the cell, you must click on the cell (place focus on the cell). You can tell when the focus is on the code cell because it will be highlighted with a thin blue box. To \"execute\" the contents of the cell, you must either hit the run button icon \u25ba or Shift-Return on your keyboard. Run the contents of the cell which contains the add-user command A message indicating that a user was successfully added will be displayed in the notebook. ['Adding password for user watsonx'] At this point you will return to the browser page which contains the watsonx.data UI. Click on the browser tab which contains the watsonx.data UI","title":"Roles and Policies"},{"location":"wxd-accesscontrol/#access-control_1","text":"To view what users are currently authorized to use the system, select the Access control icon found on the left side of the watsonx.data UI. Click on the Access Control icon on the left side of the screen A list of objects that make up the watsonx.data infrastructure are displayed. You can see that the objects are made up of: Engines Catalogs Buckets In a real-world scenario where a user will be querying data from a table, that user will need to be given a minimum of User access to an engine (to be able to run the query), User access for the catalog associated with the data (to be able to see the schema information associated with the table), and Reader access to the bucket associated with the data (to be able to read the data from object storage). Additionally, a policy has to be created to permit the user to access the table in question.","title":"Access Control"},{"location":"wxd-accesscontrol/#granting-access","text":"Select the presto-01 engine (highlighted in red) to view the current users that have access to the engine At this point, only the administrative user (ibmlhadmin) can use the Presto engine. Click on the Add Access button to add a new authorized user to the list If you do not see the watsonx userid, or the dropdown is blank, refresh the browser screen The role button has been selected in the dialog to show the role options of Admin, Manager, or User. An Admin user can grant any role to a user, while a Manager can only grant User privileges. Grant watsonx \"user\" privileges and then press Add Close the dialog by clicking on the [x] on the top right of the screen The watsonx user needs to be granted access to a catalog. In this case, the iceberg_data and hive_data catalogs are required for the Presto engine and are implicitly granted to the user. Click on the iceberg_data line to confirm that watsonx has access You should see that watsonx has already been granted access to the catalog. Close the dialog to return to the Access control screen The final step is to grant access to the underlying buckets. Because watsonx was implicitly granted access to the iceberg_data and hive_data catalogs, the underlying buckets iceberg-bucket and hive-bucket were also added to their access list. Click on iceberg-bucket to view the access control Close the dialog to return to the Access control dialog","title":"Granting Access"},{"location":"wxd-accesscontrol/#policies","text":"After access has been granted to engines, catalogs, and buckets, the final step is to create a policy to grant access to tables. Click on the Policy tab to display the current policies in place (there should be none) The Add Policy button is highlighted on the far right side of the screen. Press the Add Policy button to display the new Access Control Policy dialog Fill in the following fields with these values Policy name: selectflights Description: blank Policy status: active You can always activate a policy after you have created it. Click Next Here we need to select which catalog to use and then the schemas that the user will be able to access. Select hive_data catalog, and then the ontime schema After selecting the schema, a set of tables associated with the schema will be listed. You can choose which tables can be searched. If you choose an individual table, you can restrict which columns can be searched. Select the ontime table and then select the following columns (you will need to scroll down the page) flightdate reporting_airline flight_number_reporting_airline originairportid destairportid Press the Next button when all columns are selected The Rules dialog allows you to fine-tune what actions can be done by a user against the data. Press the Add Rule button to display the Add Rule dialog Rules can be used to Allow actions or to Deny actions. Allow watsonx to SELECT data from the data, but with no other options Note : In production versions of watsonx.data, you can provide access to a group which makes it simpler to create a set of rules that apply to a group and then add a user to a group. That way a user will inherit the rules that were applied to the group rather than having to create rules for that individual. The developer edition displays GROUP as an option, but it is not implemented. Press the Add button You can continue to add additional rules to the list. Since we only want the one rule, select the Review button. Press the Review button Confirm that the data objects and rules look correct then press the Save button The selectflights policy is now in place and is actively enforced. Press the Query Workspace Icon Before testing the policy enforcement, use the SQL icon on the left side to navigate to the hive_data catalog and view the schemas that are associated with it. Expand the ontime schema to view the tables and the columns that are available in the ontime table When you connect as watsonx , you will be able to compare what objects can be accessed from that userid.","title":"Policies"},{"location":"wxd-accesscontrol/#testing-policy-enforcement","text":"To test whether the rules are enforced, you will need to log out of the current watsonx.data UI session. At the top of the Query workspace screen, you will see the user icon on the top right. Click on the user icon to display the logout dialog Log out to the main watsonx.data UI screen You will be prompted to confirm log out. The log in dialog should be displayed. Enter the credentials for the watsonx user and press Log in Username: watsonx Password: watsonx.data Your watsonx.data UI should now display watsonx . Navigate to the SQL icon and then select hive_data \u2192 ontime \u2192 ontime You should notice that watsonx was restricted to seeing only the ontime schema in the hive_data catalog. In addition, the user was restricted to accessing one of the tables ( ontime ) and 5 columns. Attempting to run a SELECT statement against all the data will result in a policy error. Enter this text into the SQL window and Run the code SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; Correcting the SQL to include only permitted columns results in an answer set. Enter this text into the SQL window and Run the code SELECT flightdate, reporting_airline, flight_number_reporting_airline, originairportid, destairportid FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; The policy rules have been enforced for watsonx , preventing them from viewing any other schemas or tables in the system. In addition, the SQL that they could execute was restricted to specific columns in the table. Before moving onto any other sections, make sure to log out as watsonx and reconnect as the ibmlhadmin user. Click on the user icon to display the logout dialog Log out to the main watsonx.data UI screen You will be prompted to confirm log out. The log in dialog should be displayed. Enter the credentials for the ibmlhadmin user and press Log in Username: ibmlhadmin Password: password Your watsonx.data UI should now display ibmlhadmin .","title":"Testing Policy Enforcement"},{"location":"wxd-accesscontrol/#summary","text":"In this section you learned how to add a user to watsonx.data (via a command) and control what objects they are allowed to access. In production versions of watsonx.data, the adding and removal of users is directly handled through the User Interface.","title":"Summary"},{"location":"wxd-advanced/","text":"Advanced Functions Watsonx.data supports several types of functions including: Mathematical functions Conversion functions String functions Regular expression functions Window functions URL functions Geospatial functions For a complete list see - https://prestodb.io/docs/current/functions.html . We will look at using a few simple examples as part of this lab. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Concatenation of one or more string/varchar values Note: We are using a combination of the concat string function and the cast conversion function as part of this query. Concatenate two strings select concat(cast(custkey as varchar),'-',name) from iceberg_data.workshop.customer limit 2; Date functions Date functions can be used as part of the projected columns or in the predicate/where clause. Select orders from the last 2 days select orderdate from iceberg_data.workshop.orders where orderdate > date '1998-08-02' - interval '2' day; Compute the number of orders by year select distinct year(orderdate), count(orderkey) from iceberg_data.workshop.orders group by year(orderdate); \u2003 Geospatial functions There are 3 basic geometries, then some complex geometries. The basic geometries include: Points Lines Polygons Points You could use https://www.latlong.net to get the longitude/latitude given any address. SQL Representation of Co-ordinates select ST_Point(-121.748360,37.195840) as SVL, ST_Point(-122.378952, 37.621311) as SFO; Lines You could use https://www.latlong.net to get the longitude/latitude for 2 points and then create a straight line from it. Below is just a small stretch of the road leading to IBM SVL campus. SQL Representation of a Line select ST_LineFromText('LINESTRING (-121.74294303079807 37.19665657093434, -121.73659072815602 37.20102399761407)'); \u2003 Polygons You could use https://geojson.io/#map=16.39/37.196336/-121.746303 to click around and generate the coordinates for a polygon of any shape. The following is a polygon of the IBM Silicon Valley campus. SQL Representation of a Polygon select ST_Polygon('POLYGON ( (-121.74418635253568 37.196001834113844, -121.74499684288966 37.19668005184322, -121.74584008032835 37.19707784979194, -121.74629035274705 37.197645197338105, -121.74672425162339 37.198186455965086, -121.74705172247337 37.19828427337538, -121.74760023614738 37.19827775221884, -121.74848440744239 37.19836252721197, -121.74932764488139 37.19789300297414, -121.75039192514376 37.19746260319114, -121.75130884352407 37.19721479614175, -121.75195559845278 37.1963670290329, -121.75198015876644 37.19555185937345, -121.7508585711051 37.19458016564036, -121.74940132582242 37.19447582194559, -121.74841891327239 37.1942866986312, -121.7474446874937 37.193556286900346, -121.74418635253568 37.196001834113844))'); So now that we have 3 basic geometries Point, Line and Polygon we can perform different operations on spatial data including: Distance between 2 points Point in polygon Intersection of line and polygon We can now use geospatial functions in a nested way to find the distance between 2 points. Distance between SFO airport and IBM SVL select ST_Distance(to_spherical_geography(ST_Point(-122.378952, 37.621311)), to_spherical_geography(ST_Point(-121.748360,37.195840)))*0.000621371 as distance_in_miles; System Connector The Presto System connector provides information and metrics about the currently running Presto cluster. You can use this function to monitor the workloads on the Presto cluster using normal SQL queries. What queries are currently running? select * from \"system\".runtime.queries limit 5; What tasks make up a query and where is the task running? select * from \"system\".runtime.tasks limit 5; Summary In this lab you learned about some of the advanced SQL that is found in watsonx.data, including time and date functions, character formatting functions, systen functions, and a series of functions used for manipulating Geo-spatial data.","title":"Advanced Functions"},{"location":"wxd-advanced/#advanced-functions","text":"Watsonx.data supports several types of functions including: Mathematical functions Conversion functions String functions Regular expression functions Window functions URL functions Geospatial functions For a complete list see - https://prestodb.io/docs/current/functions.html . We will look at using a few simple examples as part of this lab. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side","title":"Advanced Functions"},{"location":"wxd-advanced/#concatenation-of-one-or-more-stringvarchar-values","text":"Note: We are using a combination of the concat string function and the cast conversion function as part of this query. Concatenate two strings select concat(cast(custkey as varchar),'-',name) from iceberg_data.workshop.customer limit 2;","title":"Concatenation of one or more string/varchar values"},{"location":"wxd-advanced/#date-functions","text":"Date functions can be used as part of the projected columns or in the predicate/where clause. Select orders from the last 2 days select orderdate from iceberg_data.workshop.orders where orderdate > date '1998-08-02' - interval '2' day; Compute the number of orders by year select distinct year(orderdate), count(orderkey) from iceberg_data.workshop.orders group by year(orderdate);","title":"Date functions"},{"location":"wxd-advanced/#geospatial-functions","text":"There are 3 basic geometries, then some complex geometries. The basic geometries include: Points Lines Polygons","title":"Geospatial functions"},{"location":"wxd-advanced/#points","text":"You could use https://www.latlong.net to get the longitude/latitude given any address. SQL Representation of Co-ordinates select ST_Point(-121.748360,37.195840) as SVL, ST_Point(-122.378952, 37.621311) as SFO;","title":"Points"},{"location":"wxd-advanced/#lines","text":"You could use https://www.latlong.net to get the longitude/latitude for 2 points and then create a straight line from it. Below is just a small stretch of the road leading to IBM SVL campus. SQL Representation of a Line select ST_LineFromText('LINESTRING (-121.74294303079807 37.19665657093434, -121.73659072815602 37.20102399761407)');","title":"Lines"},{"location":"wxd-advanced/#polygons","text":"You could use https://geojson.io/#map=16.39/37.196336/-121.746303 to click around and generate the coordinates for a polygon of any shape. The following is a polygon of the IBM Silicon Valley campus. SQL Representation of a Polygon select ST_Polygon('POLYGON ( (-121.74418635253568 37.196001834113844, -121.74499684288966 37.19668005184322, -121.74584008032835 37.19707784979194, -121.74629035274705 37.197645197338105, -121.74672425162339 37.198186455965086, -121.74705172247337 37.19828427337538, -121.74760023614738 37.19827775221884, -121.74848440744239 37.19836252721197, -121.74932764488139 37.19789300297414, -121.75039192514376 37.19746260319114, -121.75130884352407 37.19721479614175, -121.75195559845278 37.1963670290329, -121.75198015876644 37.19555185937345, -121.7508585711051 37.19458016564036, -121.74940132582242 37.19447582194559, -121.74841891327239 37.1942866986312, -121.7474446874937 37.193556286900346, -121.74418635253568 37.196001834113844))'); So now that we have 3 basic geometries Point, Line and Polygon we can perform different operations on spatial data including: Distance between 2 points Point in polygon Intersection of line and polygon We can now use geospatial functions in a nested way to find the distance between 2 points. Distance between SFO airport and IBM SVL select ST_Distance(to_spherical_geography(ST_Point(-122.378952, 37.621311)), to_spherical_geography(ST_Point(-121.748360,37.195840)))*0.000621371 as distance_in_miles;","title":"Polygons"},{"location":"wxd-advanced/#system-connector","text":"The Presto System connector provides information and metrics about the currently running Presto cluster. You can use this function to monitor the workloads on the Presto cluster using normal SQL queries. What queries are currently running? select * from \"system\".runtime.queries limit 5; What tasks make up a query and where is the task running? select * from \"system\".runtime.tasks limit 5;","title":"System Connector"},{"location":"wxd-advanced/#summary","text":"In this lab you learned about some of the advanced SQL that is found in watsonx.data, including time and date functions, character formatting functions, systen functions, and a series of functions used for manipulating Geo-spatial data.","title":"Summary"},{"location":"wxd-analytics/","text":"Analytic Workload This section will explore some of the analytic workloads that you can run with watsonx.data. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side For the analytic workloads, you are going to create a new schema in the iceberg_data catalog. The next statement will create the workshop schema in the existing iceberg-bucket . Create the workshop schema in the iceberg_data catalog CREATE SCHEMA IF NOT EXISTS iceberg_data.workshop with (location='s3a://iceberg-bucket/'); This SQL will create a new Apache Iceberg table using existing data in the sample Customer table as part of the TPCH catalog schema called TINY. Create the workshop customer table create table iceberg_data.workshop.customer as select * from tpch.tiny.customer; Let us start with some simple examples of running queries and analyzing the execution. Run a simple scan query which selects customer names and market segment use iceberg_data.workshop; select name, mktsegment from customer limit 3; To understand the query execution plan we use the explain statement. Run an explain against the previous statement use iceberg_data.workshop; explain select name, mktsegment from customer; What you see above is the hierarchy of logical operations to execute the query. It is very difficult to read since the information is found on one line. One option is to export the data and view the results in a spreadsheet. Explain the query and focus on IO operations use iceberg_data.workshop; explain (type io) select name, mktsegment from customer; Explain physical execution plan for the query. Explain physical execution plan for the query use iceberg_data.workshop; explain (type distributed) select name, mktsegment from customer; A fragment represents a stage of the distributed plan. The Presto scheduler schedules the execution by each stage, and stages can be run on separate instances. Rather than executing these explain queries, the SQL interface provides an explain button beside the Run button. Enter the following query into the SQL window and press the Explain button select name, mktsegment from iceberg_data.workshop.customer Click on the TableScan operator The panel on the right side provides information about the work the engine will do to retrieve the data. The Presto optimizer estimates that 1500 rows will be retrieved and take approximately 16000 cpu units. Close the explain window Another way of viewing the explain information is to run the query and then review the Query history. Run the query that is currently in the SQL window Once the query completes, switch to the Query history screen. Click on the Query History icon on the left side of the screen Refresh the browser to update the Query history Once you have refreshed the screen, look for your query in the list. It should be near the top. Click on the kebab \u22ee icon and select View Execution Plan You will see all the explain output that you created with the earlier SQL statements. Close the explain output by pressing the [x] on the screen Creating a Table with User-defined Partitions Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Tables can be partitioned in watsonx.data. This SQL will create a partitioned table, based on column mktsegment with data copied from the TPCH.TINY.CUSTOMER table. Create a partitioned table create table iceberg_data.workshop.part_customer with (partitioning = array['mktsegment']) as select * from tpch.tiny.customer; Now that have created a partitioned table, we will execute a SQL statement that will make use of this fact. Run a query against the partitioned table select * from iceberg_data.\"workshop\".part_customer where mktsegment='MACHINERY'; Due to the partitioning of this table by mktsegment , it will completely skip scanning a large percentage of the objects in the object store. Press the explain keyword in the SQL window to view the execution plan and then select the ScanFilter You should see that the optimizer expected to read only 288 rows when running this query. This is due to the table being partitioned and scans only needing to be run against one of the partitions. Close the explain window Joins and Aggregations This section will create an orders table to test joins and aggregations. Create the Orders Table create table iceberg_data.workshop.orders as select * from tpch.tiny.orders; The following SQL uses a windowing function to compute a result set. SQL with a Windowing function SELECT orderkey, clerk, totalprice, rank() OVER (PARTITION BY clerk ORDER BY totalprice DESC) AS rnk FROM iceberg_data.workshop.orders ORDER BY clerk, rnk; Prepared statements Presto provides a feature for preparing statements for repeated execution. Using this method will reduce the overhead of optimizing the statement every time you run it. Save a query as a prepared statement prepare customer_by_segment from select * from iceberg_data.workshop.customer where mktsegment=?; Once the statement has been prepared, you can execute the statement by supplying it with some parameters. In this example we are combining the two statements because the session context is not shared between SQL executions. Execute prepared statement using parameters prepare customer_by_segment from select * from iceberg_data.workshop.customer where mktsegment=?; execute customer_by_segment using 'FURNITURE'; Note that the prepared statement only exists during the current session. Summary In this lab you ran some analytic workloads, learned how to view the explain plans for a query, created a partitioned table, and created prepared statements that can be executed multiple times.","title":"Analytic Workloads"},{"location":"wxd-analytics/#analytic-workload","text":"This section will explore some of the analytic workloads that you can run with watsonx.data. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side For the analytic workloads, you are going to create a new schema in the iceberg_data catalog. The next statement will create the workshop schema in the existing iceberg-bucket . Create the workshop schema in the iceberg_data catalog CREATE SCHEMA IF NOT EXISTS iceberg_data.workshop with (location='s3a://iceberg-bucket/'); This SQL will create a new Apache Iceberg table using existing data in the sample Customer table as part of the TPCH catalog schema called TINY. Create the workshop customer table create table iceberg_data.workshop.customer as select * from tpch.tiny.customer; Let us start with some simple examples of running queries and analyzing the execution. Run a simple scan query which selects customer names and market segment use iceberg_data.workshop; select name, mktsegment from customer limit 3; To understand the query execution plan we use the explain statement. Run an explain against the previous statement use iceberg_data.workshop; explain select name, mktsegment from customer; What you see above is the hierarchy of logical operations to execute the query. It is very difficult to read since the information is found on one line. One option is to export the data and view the results in a spreadsheet. Explain the query and focus on IO operations use iceberg_data.workshop; explain (type io) select name, mktsegment from customer; Explain physical execution plan for the query. Explain physical execution plan for the query use iceberg_data.workshop; explain (type distributed) select name, mktsegment from customer; A fragment represents a stage of the distributed plan. The Presto scheduler schedules the execution by each stage, and stages can be run on separate instances. Rather than executing these explain queries, the SQL interface provides an explain button beside the Run button. Enter the following query into the SQL window and press the Explain button select name, mktsegment from iceberg_data.workshop.customer Click on the TableScan operator The panel on the right side provides information about the work the engine will do to retrieve the data. The Presto optimizer estimates that 1500 rows will be retrieved and take approximately 16000 cpu units. Close the explain window Another way of viewing the explain information is to run the query and then review the Query history. Run the query that is currently in the SQL window Once the query completes, switch to the Query history screen. Click on the Query History icon on the left side of the screen Refresh the browser to update the Query history Once you have refreshed the screen, look for your query in the list. It should be near the top. Click on the kebab \u22ee icon and select View Execution Plan You will see all the explain output that you created with the earlier SQL statements. Close the explain output by pressing the [x] on the screen","title":"Analytic Workload"},{"location":"wxd-analytics/#creating-a-table-with-user-defined-partitions","text":"Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Tables can be partitioned in watsonx.data. This SQL will create a partitioned table, based on column mktsegment with data copied from the TPCH.TINY.CUSTOMER table. Create a partitioned table create table iceberg_data.workshop.part_customer with (partitioning = array['mktsegment']) as select * from tpch.tiny.customer; Now that have created a partitioned table, we will execute a SQL statement that will make use of this fact. Run a query against the partitioned table select * from iceberg_data.\"workshop\".part_customer where mktsegment='MACHINERY'; Due to the partitioning of this table by mktsegment , it will completely skip scanning a large percentage of the objects in the object store. Press the explain keyword in the SQL window to view the execution plan and then select the ScanFilter You should see that the optimizer expected to read only 288 rows when running this query. This is due to the table being partitioned and scans only needing to be run against one of the partitions. Close the explain window","title":"Creating a Table with User-defined Partitions"},{"location":"wxd-analytics/#joins-and-aggregations","text":"This section will create an orders table to test joins and aggregations. Create the Orders Table create table iceberg_data.workshop.orders as select * from tpch.tiny.orders; The following SQL uses a windowing function to compute a result set. SQL with a Windowing function SELECT orderkey, clerk, totalprice, rank() OVER (PARTITION BY clerk ORDER BY totalprice DESC) AS rnk FROM iceberg_data.workshop.orders ORDER BY clerk, rnk;","title":"Joins and Aggregations"},{"location":"wxd-analytics/#prepared-statements","text":"Presto provides a feature for preparing statements for repeated execution. Using this method will reduce the overhead of optimizing the statement every time you run it. Save a query as a prepared statement prepare customer_by_segment from select * from iceberg_data.workshop.customer where mktsegment=?; Once the statement has been prepared, you can execute the statement by supplying it with some parameters. In this example we are combining the two statements because the session context is not shared between SQL executions. Execute prepared statement using parameters prepare customer_by_segment from select * from iceberg_data.workshop.customer where mktsegment=?; execute customer_by_segment using 'FURNITURE'; Note that the prepared statement only exists during the current session.","title":"Prepared statements"},{"location":"wxd-analytics/#summary","text":"In this lab you ran some analytic workloads, learned how to view the explain plans for a query, created a partitioned table, and created prepared statements that can be executed multiple times.","title":"Summary"},{"location":"wxd-datamanager/","text":"Data Manager The Data manager page can be used to explore and curate your data. It includes a data objects navigation pane on the left side of the page with a navigable hierarchy of engine \u2192 catalog \u2192 schema \u2192 table. Click on the Data Manager icon on the left side of the screen When the Data manager initially starts, you may not see any values on the right-hand side of the screen. This side of the screen is populated when you expand one of the selections on the left side. Watsonx.data (Presto) organizes tables, views, and other database objects in schemas. A schema is a logical collection or container of related database objects. For example, sales tables might be contained in one schema and marketing tables might be contained in another. The top-level navigation point is the query engine. You start by selecting an engine that is associated with the catalog and bucket you want to manage. As there is only one engine in this environment (presto-01), it is selected by default. If this was an environment with multiple engines defined, you would have the choice of selecting any one of the engines you have set up (as the administrator) or that you\u2019ve been given access to (as a non-administrator). With the engine selected, you can now navigate through the catalogs associated with the selected engine (the catalogs are listed in the Catalogs associated section on the left). Currently, this includes the two default catalogs ( hive_data and iceberg_data ) and the system catalog ( wxd_system_data ). Press the \u25ba beside the hive_data catalog to display the schemas in the catalog The system has automatically expanded the first schema in the list ( gosalesdw ) and displayed the tables that are associated with that schema. Collapse the gosalesdw schema by pressing the \u25bc icon Your screen may show fewer schemas, but should include gosalesdw , taxi , and ontime in the list. Expand the ontime schema by pressing \u25ba beside the schema name There are 5 tables associated with this schema. Get details of the ontime table by pressing on the table name The ontime table provides details of flight delays for flights within the United States for a one-month period of time. This screen displays the table definition (DDL) for the data. The panel on the right contains additional tabs which provide options for you to view some sample data, and to generate the DDL (Data Definition Language) SQL for the table. Select Data sample tab Select DDL (Data Definition) tab You can use the generated DDL as an SQL statement that can be used to recreate the table. Summary This lab covered the Data Manager menu found in the watsonx.data UI. The Data manager page can be used to explore and curate your data. It includes a data objects navigation pane on the left side of the page with a navigable hierarchy. You learned how to: Explore the catalogs and schemas registered in the system View the structure (definition) of a table View a subset of data in the table Generate the DDL for creating the table The next section will explore the Query workspace and how to issue SQL commands against the tables registered in the system.","title":"Data Manager"},{"location":"wxd-datamanager/#data-manager","text":"The Data manager page can be used to explore and curate your data. It includes a data objects navigation pane on the left side of the page with a navigable hierarchy of engine \u2192 catalog \u2192 schema \u2192 table. Click on the Data Manager icon on the left side of the screen When the Data manager initially starts, you may not see any values on the right-hand side of the screen. This side of the screen is populated when you expand one of the selections on the left side. Watsonx.data (Presto) organizes tables, views, and other database objects in schemas. A schema is a logical collection or container of related database objects. For example, sales tables might be contained in one schema and marketing tables might be contained in another. The top-level navigation point is the query engine. You start by selecting an engine that is associated with the catalog and bucket you want to manage. As there is only one engine in this environment (presto-01), it is selected by default. If this was an environment with multiple engines defined, you would have the choice of selecting any one of the engines you have set up (as the administrator) or that you\u2019ve been given access to (as a non-administrator). With the engine selected, you can now navigate through the catalogs associated with the selected engine (the catalogs are listed in the Catalogs associated section on the left). Currently, this includes the two default catalogs ( hive_data and iceberg_data ) and the system catalog ( wxd_system_data ). Press the \u25ba beside the hive_data catalog to display the schemas in the catalog The system has automatically expanded the first schema in the list ( gosalesdw ) and displayed the tables that are associated with that schema. Collapse the gosalesdw schema by pressing the \u25bc icon Your screen may show fewer schemas, but should include gosalesdw , taxi , and ontime in the list. Expand the ontime schema by pressing \u25ba beside the schema name There are 5 tables associated with this schema. Get details of the ontime table by pressing on the table name The ontime table provides details of flight delays for flights within the United States for a one-month period of time. This screen displays the table definition (DDL) for the data. The panel on the right contains additional tabs which provide options for you to view some sample data, and to generate the DDL (Data Definition Language) SQL for the table. Select Data sample tab Select DDL (Data Definition) tab You can use the generated DDL as an SQL statement that can be used to recreate the table.","title":"Data Manager"},{"location":"wxd-datamanager/#summary","text":"This lab covered the Data Manager menu found in the watsonx.data UI. The Data manager page can be used to explore and curate your data. It includes a data objects navigation pane on the left side of the page with a navigable hierarchy. You learned how to: Explore the catalogs and schemas registered in the system View the structure (definition) of a table View a subset of data in the table Generate the DDL for creating the table The next section will explore the Query workspace and how to issue SQL commands against the tables registered in the system.","title":"Summary"},{"location":"wxd-disclaimer/","text":"Disclaimer Watson.data Copyright \u00a9 2024 by International Business Machines Corporation (IBM). All rights reserved. Printed in Canada. Except as permitted under the Copyright Act of 1976, no part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of IBM, with the exception that the program listings may be entered, stored, and executed in a computer system, but they may not be reproduced for publication. The contents of this lab represent those features that may or may not be available in the current release of any products mentioned within this lab despite what the lab may say. IBM reserves the right to include or exclude any functionality mentioned in this lab for the current release of watsonx.data, or a subsequent release. In addition, any claims made in this lab are not official communications by IBM; rather, they are observed by the authors in unaudited testing and research. The views expressed in this lab is those of the authors and not necessarily those of the IBM Corporation; both are not liable for any of the claims, assertions, or contents in this lab. IBM's statements regarding its plans, directions, and intent are subject to change or withdrawal without notice and at IBM's sole discretion. Information regarding potential future products is intended to outline our general product direction and it should not be relied on in making a purchasing decision. The information mentioned regarding potential future products is not a commitment, promise, or legal obligation to deliver any material, code, or functionality. Information about potential future products may not be incorporated into any contract. The development, release, and timing of any future feature or functionality described for our products remains at our sole discretion. Performance is based on measurements and projections using standard IBM benchmarks in a controlled environment. The actual throughput or performance that any user will experience will vary depending upon many factors, including considerations such as the amount of multiprogramming in the user's job stream, the I/O configuration, the storage configuration, and the workload processed. Therefore, no assurance can be given that an individual user will achieve results like those stated here. U.S. Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM. Information in this eBook (including information relating to products that have not yet been announced by IBM) has been reviewed for accuracy as of the date of initial publication and could include unintentional technical or typographical errors. IBM shall have no responsibility to update this information. THIS DOCUMENT IS DISTRIBUTED \"AS IS\" WITHOUT ANY WARRANTY, EITHER EXPRESS OR IMPLIED. IN NO EVENT SHALL IBM BE LIABLE FOR ANY DAMAGE ARISING FROM THE USE OF THIS INFORMATION, INCLUDING BUT NOT LIMITED TO, LOSS OF DATA, BUSINESS INTERRUPTION, LOSS OF PROFIT OR LOSS OF OPPORTUNITY. IBM products and services are warranted according to the terms and conditions of the agreements under which they are provided. References in this document to IBM products, programs, or services does not imply that IBM intends to make such products, programs, or services available in all countries in which IBM operates or does business. Information concerning non-IBM products was obtained from the suppliers of those products, their published announcements, or other publicly available sources. IBM has not tested those products in connection with this publication and cannot confirm the accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the capabilities of non-IBM products should be addressed to the suppliers of those products. IBM does not warrant the quality of any third-party products, or the ability of any such third-party products to interoperate with IBM's products. IBM EXPRESSLY DISCLAIMS ALL WARRANTIES, EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. The provision of the information contained herein is not intended to, and does not, grant any right or license under any IBM patents, copyrights, trademarks, or other intellectual property right. IBM, the IBM logo, ibm.com, Aspera\u00ae, Bluemix, Blueworks Live, CICS, Clearcase, Cognos\u00ae, DOORS\u00ae, Emptoris\u00ae, Enterprise Document Management System\u2122, FASP\u00ae, FileNet\u00ae, Global Business Services \u00ae, Global Technology Services \u00ae, IBM ExperienceOne\u2122, IBM SmartCloud\u00ae, IBM Social Business\u00ae, Information on Demand, ILOG, Maximo\u00ae, MQIntegrator\u00ae, MQSeries\u00ae, Netcool\u00ae, OMEGAMON, OpenPower, PureAnalytics\u2122, PureApplication\u00ae, pureCluster\u2122, PureCoverage\u00ae, PureData\u00ae, PureExperience\u00ae, PureFlex\u00ae, pureQuery\u00ae, pureScale\u00ae, PureSystems\u00ae, QRadar\u00ae, Rational\u00ae, Rhapsody\u00ae, Smarter Commerce\u00ae, SoDA, SPSS, Sterling Commerce\u00ae, StoredIQ, Tealeaf\u00ae, Tivoli\u00ae, Trusteer\u00ae, Unica\u00ae, urban{code}\u00ae, Watson, WebSphere\u00ae, Worklight\u00ae, X-Force\u00ae and System z\u00ae Z/OS, are trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the Web at \"Copyright and trademark information\" at: www.ibm.com/legal/copytrade.shtml. All trademarks or copyrights mentioned herein are the possession of their respective owners and IBM makes no claim of ownership by the mention of products that contain these marks.","title":"Disclaimer"},{"location":"wxd-disclaimer/#disclaimer","text":"","title":"Disclaimer"},{"location":"wxd-disclaimer/#watsondata","text":"Copyright \u00a9 2024 by International Business Machines Corporation (IBM). All rights reserved. Printed in Canada. Except as permitted under the Copyright Act of 1976, no part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of IBM, with the exception that the program listings may be entered, stored, and executed in a computer system, but they may not be reproduced for publication. The contents of this lab represent those features that may or may not be available in the current release of any products mentioned within this lab despite what the lab may say. IBM reserves the right to include or exclude any functionality mentioned in this lab for the current release of watsonx.data, or a subsequent release. In addition, any claims made in this lab are not official communications by IBM; rather, they are observed by the authors in unaudited testing and research. The views expressed in this lab is those of the authors and not necessarily those of the IBM Corporation; both are not liable for any of the claims, assertions, or contents in this lab. IBM's statements regarding its plans, directions, and intent are subject to change or withdrawal without notice and at IBM's sole discretion. Information regarding potential future products is intended to outline our general product direction and it should not be relied on in making a purchasing decision. The information mentioned regarding potential future products is not a commitment, promise, or legal obligation to deliver any material, code, or functionality. Information about potential future products may not be incorporated into any contract. The development, release, and timing of any future feature or functionality described for our products remains at our sole discretion. Performance is based on measurements and projections using standard IBM benchmarks in a controlled environment. The actual throughput or performance that any user will experience will vary depending upon many factors, including considerations such as the amount of multiprogramming in the user's job stream, the I/O configuration, the storage configuration, and the workload processed. Therefore, no assurance can be given that an individual user will achieve results like those stated here. U.S. Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM. Information in this eBook (including information relating to products that have not yet been announced by IBM) has been reviewed for accuracy as of the date of initial publication and could include unintentional technical or typographical errors. IBM shall have no responsibility to update this information. THIS DOCUMENT IS DISTRIBUTED \"AS IS\" WITHOUT ANY WARRANTY, EITHER EXPRESS OR IMPLIED. IN NO EVENT SHALL IBM BE LIABLE FOR ANY DAMAGE ARISING FROM THE USE OF THIS INFORMATION, INCLUDING BUT NOT LIMITED TO, LOSS OF DATA, BUSINESS INTERRUPTION, LOSS OF PROFIT OR LOSS OF OPPORTUNITY. IBM products and services are warranted according to the terms and conditions of the agreements under which they are provided. References in this document to IBM products, programs, or services does not imply that IBM intends to make such products, programs, or services available in all countries in which IBM operates or does business. Information concerning non-IBM products was obtained from the suppliers of those products, their published announcements, or other publicly available sources. IBM has not tested those products in connection with this publication and cannot confirm the accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the capabilities of non-IBM products should be addressed to the suppliers of those products. IBM does not warrant the quality of any third-party products, or the ability of any such third-party products to interoperate with IBM's products. IBM EXPRESSLY DISCLAIMS ALL WARRANTIES, EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. The provision of the information contained herein is not intended to, and does not, grant any right or license under any IBM patents, copyrights, trademarks, or other intellectual property right. IBM, the IBM logo, ibm.com, Aspera\u00ae, Bluemix, Blueworks Live, CICS, Clearcase, Cognos\u00ae, DOORS\u00ae, Emptoris\u00ae, Enterprise Document Management System\u2122, FASP\u00ae, FileNet\u00ae, Global Business Services \u00ae, Global Technology Services \u00ae, IBM ExperienceOne\u2122, IBM SmartCloud\u00ae, IBM Social Business\u00ae, Information on Demand, ILOG, Maximo\u00ae, MQIntegrator\u00ae, MQSeries\u00ae, Netcool\u00ae, OMEGAMON, OpenPower, PureAnalytics\u2122, PureApplication\u00ae, pureCluster\u2122, PureCoverage\u00ae, PureData\u00ae, PureExperience\u00ae, PureFlex\u00ae, pureQuery\u00ae, pureScale\u00ae, PureSystems\u00ae, QRadar\u00ae, Rational\u00ae, Rhapsody\u00ae, Smarter Commerce\u00ae, SoDA, SPSS, Sterling Commerce\u00ae, StoredIQ, Tealeaf\u00ae, Tivoli\u00ae, Trusteer\u00ae, Unica\u00ae, urban{code}\u00ae, Watson, WebSphere\u00ae, Worklight\u00ae, X-Force\u00ae and System z\u00ae Z/OS, are trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the Web at \"Copyright and trademark information\" at: www.ibm.com/legal/copytrade.shtml. All trademarks or copyrights mentioned herein are the possession of their respective owners and IBM makes no claim of ownership by the mention of products that contain these marks.","title":"Watson.data"},{"location":"wxd-federation/","text":"Federation with watsonx.data Watsonx.data can federate data from other data sources, there are a few out of box connectors and one could create additional connectors using the SDK if need be. We will be use a Db2 instance to retrieve data from that system. Adding Db2 to watsonx.data To set up federation, we need to catalog the database server to the watsonx.data system. In order to do that we will need to use the Infrastructure manager. Click on the Infrastructure icon on the left side of the screen The Infrastructure manager view should be similar to this screen. On the top right-hand corner, select Add Component \u2192 Add Database The Add database dialog is displayed. Enter the following values into the dialog Database type \u2013 IBM Db2 Database name \u2013 gosales Hostname \u2013 watsonxdata Port \u2013 50000 Display name \u2013 gosales Username \u2013 db2inst1 Password \u2013 db2inst1 Catalog Name \u2013 gosales Your screen should look like the one below. Press the Test connection button to check connectivity Once you are satisfied with the settings, press Register Your infrastructure screen should now look similar to the following. What we are currently missing the connection between the Presto engine and the Db2 data in gosales. We must connect the Db2 database to the Presto engine. Use your mouse to hover over the Db2 icon until you see the Associate connection icon Click on the Manage associations icon You should see the following confirmation dialog. Select the presto-01 engine and press Save and restart engine The infrastructure display will refresh to show the Db2 connection. Presto Federation Presto Engine Status When a new database or bucket is added to the watsonx.data system, the Presto engine will become unavailable temporarily while it catalogs the new objects. During this time any SQL requests or table browsing may result in an error code being displayed. The system usually restarts within a minute and you can attempt your SQL again. We start by checking the status of our new connection to Db2. Click on the Data Manager icon on the left side of the screen Your screen should look similar to this one. Select the Db2 catalog and expand the gosales and gosalesdw objects Select the DIST_INVENTORY_FACT table Watsonx.data is able to view the Db2 catalog and query the data that is found in it. We can now add Db2 to the queries that we run in the Presto engine. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Select the names of employees in the GOSALES database with employee ids between 4000 and 5000 select go.\"EMPLOYEE_KEY\", go.\"FIRST_NAME\", go.\"LAST_NAME\" from gosales.\"GOSALESDW\".\"EMP_EMPLOYEE_DIM\" go where go.\"EMPLOYEE_KEY\" between 4000 and 5000 order by go.\"EMPLOYEE_KEY\" Now we can create a query that combines data from the TPCH catalog and the Db2 catalog. Select TPCH customers who are employees in GOSALES select t1.custkey, go.\"FIRST_NAME\", go.\"LAST_NAME\" from tpch.sf1.customer t1, gosales.\"GOSALESDW\".\"EMP_EMPLOYEE_DIM\" go where t1.custkey between 4000 and 5000 AND go.\"EMPLOYEE_KEY\" = t1.custkey order by t1.custkey We can use the federation capability to offload tables from Db2 into a watsonx.data table. Create a table in iceberg_data using the Db2 employee table create table iceberg_data.workshop.db2employee AS select go.\"EMPLOYEE_KEY\", go.\"FIRST_NAME\", go.\"LAST_NAME\" from gosales.\"GOSALESDW\".\"EMP_EMPLOYEE_DIM\" go where go.\"EMPLOYEE_KEY\" between 4000 and 5000 order by go.\"EMPLOYEE_KEY\" Now we can query the data natively with the Presto engine. Query the contents of the new db2employee table select * from iceberg_data.workshop.db2employee Summary In this lab you learned about the federation capabilities of the Presto engine, how to register a database into the system, and how you can run queries and create tables using Federation.","title":"Federation"},{"location":"wxd-federation/#federation-with-watsonxdata","text":"Watsonx.data can federate data from other data sources, there are a few out of box connectors and one could create additional connectors using the SDK if need be. We will be use a Db2 instance to retrieve data from that system.","title":"Federation with watsonx.data"},{"location":"wxd-federation/#adding-db2-to-watsonxdata","text":"To set up federation, we need to catalog the database server to the watsonx.data system. In order to do that we will need to use the Infrastructure manager. Click on the Infrastructure icon on the left side of the screen The Infrastructure manager view should be similar to this screen. On the top right-hand corner, select Add Component \u2192 Add Database The Add database dialog is displayed. Enter the following values into the dialog Database type \u2013 IBM Db2 Database name \u2013 gosales Hostname \u2013 watsonxdata Port \u2013 50000 Display name \u2013 gosales Username \u2013 db2inst1 Password \u2013 db2inst1 Catalog Name \u2013 gosales Your screen should look like the one below. Press the Test connection button to check connectivity Once you are satisfied with the settings, press Register Your infrastructure screen should now look similar to the following. What we are currently missing the connection between the Presto engine and the Db2 data in gosales. We must connect the Db2 database to the Presto engine. Use your mouse to hover over the Db2 icon until you see the Associate connection icon Click on the Manage associations icon You should see the following confirmation dialog. Select the presto-01 engine and press Save and restart engine The infrastructure display will refresh to show the Db2 connection.","title":"Adding Db2 to watsonx.data"},{"location":"wxd-federation/#presto-federation","text":"Presto Engine Status When a new database or bucket is added to the watsonx.data system, the Presto engine will become unavailable temporarily while it catalogs the new objects. During this time any SQL requests or table browsing may result in an error code being displayed. The system usually restarts within a minute and you can attempt your SQL again. We start by checking the status of our new connection to Db2. Click on the Data Manager icon on the left side of the screen Your screen should look similar to this one. Select the Db2 catalog and expand the gosales and gosalesdw objects Select the DIST_INVENTORY_FACT table Watsonx.data is able to view the Db2 catalog and query the data that is found in it. We can now add Db2 to the queries that we run in the Presto engine. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Select the names of employees in the GOSALES database with employee ids between 4000 and 5000 select go.\"EMPLOYEE_KEY\", go.\"FIRST_NAME\", go.\"LAST_NAME\" from gosales.\"GOSALESDW\".\"EMP_EMPLOYEE_DIM\" go where go.\"EMPLOYEE_KEY\" between 4000 and 5000 order by go.\"EMPLOYEE_KEY\" Now we can create a query that combines data from the TPCH catalog and the Db2 catalog. Select TPCH customers who are employees in GOSALES select t1.custkey, go.\"FIRST_NAME\", go.\"LAST_NAME\" from tpch.sf1.customer t1, gosales.\"GOSALESDW\".\"EMP_EMPLOYEE_DIM\" go where t1.custkey between 4000 and 5000 AND go.\"EMPLOYEE_KEY\" = t1.custkey order by t1.custkey We can use the federation capability to offload tables from Db2 into a watsonx.data table. Create a table in iceberg_data using the Db2 employee table create table iceberg_data.workshop.db2employee AS select go.\"EMPLOYEE_KEY\", go.\"FIRST_NAME\", go.\"LAST_NAME\" from gosales.\"GOSALESDW\".\"EMP_EMPLOYEE_DIM\" go where go.\"EMPLOYEE_KEY\" between 4000 and 5000 order by go.\"EMPLOYEE_KEY\" Now we can query the data natively with the Presto engine. Query the contents of the new db2employee table select * from iceberg_data.workshop.db2employee","title":"Presto Federation"},{"location":"wxd-federation/#summary","text":"In this lab you learned about the federation capabilities of the Presto engine, how to register a database into the system, and how you can run queries and create tables using Federation.","title":"Summary"},{"location":"wxd-glossary/","text":"Glossary Apache Superset : Apache Superset is an open-source software application for data exploration and data visualization able to handle data at petabyte scale. Apache Superset is a modern, enterprise-ready business intelligence web application. It is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple pie charts to highly detailed geospatial charts. Application Programming Interface (API)**: Application Programming Interface (API) is a programmatic interface for executing functions of an application in an automated or manual fashion without using a CLI or User Interface. Buckets : Buckets are the basic containers that hold your data. Everything that you store in Cloud Storage must be contained in a bucket. You can use buckets to organize your data and control access to your data, but unlike directories and folders, you cannot nest buckets. Catalog : This term may have many meanings depending on context. Review below: Service Catalog - A service catalog is a comprehensive list of cloud computing services that an organization offers its customers. The catalog is the only portion of the company's service portfolio that is published and provided to customers as a support to the sale or delivery of offered services. Data Catalog - A collection of business information describing the available datasets within an organization. Metastore Catalog - A collection of technical and operational metadata allowing a query engine to overlay a virtual table on a collection of discrete data files. Connector Catalog - The named representation of a connector within the virtual warehouse of a presto instance. Command Line Interface (CLI) : A command-line interface (CLI) is a text-based user interface (UI) used to run programs, manage computer files and interact with the computer. dBeaver : DBeaver is a SQL client software application and a database administration tool. For relational databases it uses the JDBC application programming interface to interact with databases via a JDBC driver. For other databases it uses proprietary database drivers. Federation : A federated database is a system in which several databases appear to function as a single entity. Each component database in the system is completely self-sustained and functional. When an application queries the federated database, the system figures out which of its component databases contains the data being requested and passes the request to it. Federated databases can be thought of as database virtualization in much the same way that storage virtualization makes several drives appear as one. MinIO : MinIO is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It runs on-prem and on any cloud (public or private) and from the data center to the edge. MinIO is software-defined and open source under GNU AGPL v3. Object Storage : Object storage is a data storage architecture for storing unstructured data, which sections data into units\u2014objects\u2014and stores them in a structurally flat data environment. Each object includes the data, metadata, and a unique identifier that applications can use for easy access and retrieval. Presto : Presto is a distributed database query engine (written in Java) that uses the SQL query language. Its architecture allows users to query data sources such as Hadoop, Cassandra, Kafka, AWS S3, Alluxio, MySQL, MongoDB and Teradata, and allows use of multiple data sources within a query. Presto is community-driven open-source software released under the Apache License. Presto's architecture is very similar to other database management systems using cluster computing, sometimes called massively parallel processing (MPP). SPARK : Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Spark can be used with watsonx.data but is not included in the watsonx.data environment image provided. TechZone (IBM Technology Zone) : IBM Technology Zone is the platform where the developer edition of watsonx.data with the sample data sets has been provisioned. Generally, it allows Go To Market teams and Business Partners to easily build technical 'Show Me' live environments, POTs, prototypes, and MVPs, which can then be customized and shared with peers and customers to experience IBM Technology.","title":"Glossary"},{"location":"wxd-glossary/#glossary","text":"Apache Superset : Apache Superset is an open-source software application for data exploration and data visualization able to handle data at petabyte scale. Apache Superset is a modern, enterprise-ready business intelligence web application. It is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple pie charts to highly detailed geospatial charts. Application Programming Interface (API)**: Application Programming Interface (API) is a programmatic interface for executing functions of an application in an automated or manual fashion without using a CLI or User Interface. Buckets : Buckets are the basic containers that hold your data. Everything that you store in Cloud Storage must be contained in a bucket. You can use buckets to organize your data and control access to your data, but unlike directories and folders, you cannot nest buckets. Catalog : This term may have many meanings depending on context. Review below: Service Catalog - A service catalog is a comprehensive list of cloud computing services that an organization offers its customers. The catalog is the only portion of the company's service portfolio that is published and provided to customers as a support to the sale or delivery of offered services. Data Catalog - A collection of business information describing the available datasets within an organization. Metastore Catalog - A collection of technical and operational metadata allowing a query engine to overlay a virtual table on a collection of discrete data files. Connector Catalog - The named representation of a connector within the virtual warehouse of a presto instance. Command Line Interface (CLI) : A command-line interface (CLI) is a text-based user interface (UI) used to run programs, manage computer files and interact with the computer. dBeaver : DBeaver is a SQL client software application and a database administration tool. For relational databases it uses the JDBC application programming interface to interact with databases via a JDBC driver. For other databases it uses proprietary database drivers. Federation : A federated database is a system in which several databases appear to function as a single entity. Each component database in the system is completely self-sustained and functional. When an application queries the federated database, the system figures out which of its component databases contains the data being requested and passes the request to it. Federated databases can be thought of as database virtualization in much the same way that storage virtualization makes several drives appear as one. MinIO : MinIO is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It runs on-prem and on any cloud (public or private) and from the data center to the edge. MinIO is software-defined and open source under GNU AGPL v3. Object Storage : Object storage is a data storage architecture for storing unstructured data, which sections data into units\u2014objects\u2014and stores them in a structurally flat data environment. Each object includes the data, metadata, and a unique identifier that applications can use for easy access and retrieval. Presto : Presto is a distributed database query engine (written in Java) that uses the SQL query language. Its architecture allows users to query data sources such as Hadoop, Cassandra, Kafka, AWS S3, Alluxio, MySQL, MongoDB and Teradata, and allows use of multiple data sources within a query. Presto is community-driven open-source software released under the Apache License. Presto's architecture is very similar to other database management systems using cluster computing, sometimes called massively parallel processing (MPP). SPARK : Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Spark can be used with watsonx.data but is not included in the watsonx.data environment image provided. TechZone (IBM Technology Zone) : IBM Technology Zone is the platform where the developer edition of watsonx.data with the sample data sets has been provisioned. Generally, it allows Go To Market teams and Business Partners to easily build technical 'Show Me' live environments, POTs, prototypes, and MVPs, which can then be customized and shared with peers and customers to experience IBM Technology.","title":"Glossary"},{"location":"wxd-infrastructure/","text":"Infrastructure Manager The Infrastructure manager page opens with a graphical canvas view of the different infrastructure components currently defined in this watsonx.data environment. Before describing the contents of the UI, some explanation of the watsonx.data objects are necessary. Watsonx.data is based on open-source Presto DB, a distributed query engine that enables querying data stored in open file formats using open table formats for optimization and performance. There are four types of objects that are commonly referred to in the Presto DB environment: Engine \u2013 Each watsonx.data installation has at least one Presto DB engine which is used for querying the data. An installation can have more than one Presto engine to improve performance, isolate workloads, or test different versions. Additional engines can include Apache Spark, which provides capabilities for reading and transforming many types of data. Catalogs \u2013 Catalogs contain information about tables, files, and objects that are found in databases or file systems. Catalogs can be specific to the type of object being manipulated (Apache Hive, Apache Iceberg). Buckets \u2013 Buckets refer to Cloud Object Storage (MinIO, AWS S3, IBM COS, etc...) that contain the files and objects that you will catalog and eventually query with an engine. Database \u2013 These represent 3rd party databases that are used to query tables that are in a vendor\u2019s proprietary format (Oracle, Db2, Netezza, etc...). There are two built-in catalogs provided with watsonx.data: Hive_data \u2013 This catalog contains references to objects that are immutable. These are objects such as CSV (comma-separated values), Text, JSON, and Apache Parquet files which are read-only. Iceberg_data \u2013 This catalog contains references to tables that are in Apache Iceberg format. These tables are optimized for space usage and query performance and can be updated. Now that you know what the various components are, we can look at the information found in the Infrastructure manager. Infrastructure Manager UI The Infrastructure manager page opens with a graphical canvas view of the different infrastructure components currently defined in this watsonx.data environment. Click on the Infrastructure icon on the left side of the screen The Infrastructure manager provides a helpful visualization of the current system. These components are: Engines (blue layer) \u2013 The query engines that will access the data. Catalogs (purple layer) \u2013 Catalogs contain metadata about the objects found in buckets or databases. Each catalog is associated with one or more engines. An engine can\u2019t access data in a bucket or a remote database unless the corresponding catalog is associated with the engine. Buckets (green layer) \u2013 An object-store that contains data (i.e., MinIO, AWS S3, Cloud-Object Storage). Each bucket is associated with a catalog (with a 1:1 mapping). When a bucket is added to watsonx.data, a catalog is created for it at the same time, based on input from the user. Databases (blue layer) \u2013 Specialty Database engines. If a database connection is added (for federation purposes), a catalog is created for that database connection as well. The diagram can become complex as the number of data sources and engines increases. You can use the magnifier in the bottom right corner to fit the diagram onto the screen or to focus in on a portion of the diagram. You have the option of auto-arranging, zooming in, zooming out, or fitting the diagram to the screen. Click on the Zoom out (-) button and then Zoom in (+) The following is an example of a complex system where the diagram was fit to the screen. You can also use your mouse to click on part of the screen and \u201cmove\u201d it to center the diagram. To reduce the objects on the screen, you can filter them by object type using the filter icon: Filter by Catalogs to view the three that exist in the system To reset the screen back to the full diagram, choose reset in the filter dialog. Reset the Filters You can view the Infrastructure diagram as table by selecting the list view icon at the top right of the screen. Select List View Selecting the list icon will display the system as a table with a separate tab for Engines, Catalogs, Buckets, and Databases. Click on each tab in the table view to see the contents Reset back to the topology view. Switch back to topology view to the show the entire diagram Exploring the Contents of a Catalog or Bucket Hover your mouse over the hive_data catalog As you hover your mouse over the hive_data catalog in the topology view, the elements (engine, catalogs, buckets, databases) that are connected will be highlighted on the screen. To explore the contents of the hive_data catalog, click on the catalog icon. Click on the hive_data catalog icon Detailed information about the catalog is displayed on the screen. There are additional tabs at the top of the screen that provide information on the data objects that are cataloged and the access control for those objects. Click on the Data Objects tab The Data Objects tab provides detailed information on the contents of the catalog. This provides an easy way to explore the contents of the catalog. Click on the Access Control Tab To close this dialog, press the [x] in the top right corner of the list. Close the dialog by pressing the [x] in the corner You should now see the topology view of the system. Adding Resources to watsonx.data To view how engines, buckets, and databases can be added, click on the Add component on the top right of the Infrastructure screen and select Create Engine. Select Add component -> Database A dialog will appear that provides options for adding another bucket or database to the system. A production version of watsonx.data would also include an option for adding another compute engine to the system. The Database Type drop-down menu supports a number of databases. Select the database type drop-down The watsonx.data UI supports many types of databases that can be cataloged in the system. This list continues to expand with every release of the product. Press Cancel to return to the Infrastructure Screen Summary The Infrastructure view provides a way to visualize the topology of the watsonx.data system. The topology includes: Engines (blue layer) \u2013 The query engines that will access the data. Catalogs (purple layer) \u2013 Catalogs contain metadata about the objects found in buckets or databases. Buckets (green layer) \u2013 An object-store that contains data (i.e., MinIO, AWS S3, Cloud-Object Storage). Databases (blue layer) \u2013 Specialty Database engines. The Infrastructure view allows an administrator to add Engines, Catalogs, Buckets, and Databases to the system. In addition, the individual objects can be selected to view the definition, contents, and access control.","title":"Infrastructure Manager"},{"location":"wxd-infrastructure/#infrastructure-manager","text":"The Infrastructure manager page opens with a graphical canvas view of the different infrastructure components currently defined in this watsonx.data environment. Before describing the contents of the UI, some explanation of the watsonx.data objects are necessary. Watsonx.data is based on open-source Presto DB, a distributed query engine that enables querying data stored in open file formats using open table formats for optimization and performance. There are four types of objects that are commonly referred to in the Presto DB environment: Engine \u2013 Each watsonx.data installation has at least one Presto DB engine which is used for querying the data. An installation can have more than one Presto engine to improve performance, isolate workloads, or test different versions. Additional engines can include Apache Spark, which provides capabilities for reading and transforming many types of data. Catalogs \u2013 Catalogs contain information about tables, files, and objects that are found in databases or file systems. Catalogs can be specific to the type of object being manipulated (Apache Hive, Apache Iceberg). Buckets \u2013 Buckets refer to Cloud Object Storage (MinIO, AWS S3, IBM COS, etc...) that contain the files and objects that you will catalog and eventually query with an engine. Database \u2013 These represent 3rd party databases that are used to query tables that are in a vendor\u2019s proprietary format (Oracle, Db2, Netezza, etc...). There are two built-in catalogs provided with watsonx.data: Hive_data \u2013 This catalog contains references to objects that are immutable. These are objects such as CSV (comma-separated values), Text, JSON, and Apache Parquet files which are read-only. Iceberg_data \u2013 This catalog contains references to tables that are in Apache Iceberg format. These tables are optimized for space usage and query performance and can be updated. Now that you know what the various components are, we can look at the information found in the Infrastructure manager.","title":"Infrastructure Manager"},{"location":"wxd-infrastructure/#infrastructure-manager-ui","text":"The Infrastructure manager page opens with a graphical canvas view of the different infrastructure components currently defined in this watsonx.data environment. Click on the Infrastructure icon on the left side of the screen The Infrastructure manager provides a helpful visualization of the current system. These components are: Engines (blue layer) \u2013 The query engines that will access the data. Catalogs (purple layer) \u2013 Catalogs contain metadata about the objects found in buckets or databases. Each catalog is associated with one or more engines. An engine can\u2019t access data in a bucket or a remote database unless the corresponding catalog is associated with the engine. Buckets (green layer) \u2013 An object-store that contains data (i.e., MinIO, AWS S3, Cloud-Object Storage). Each bucket is associated with a catalog (with a 1:1 mapping). When a bucket is added to watsonx.data, a catalog is created for it at the same time, based on input from the user. Databases (blue layer) \u2013 Specialty Database engines. If a database connection is added (for federation purposes), a catalog is created for that database connection as well. The diagram can become complex as the number of data sources and engines increases. You can use the magnifier in the bottom right corner to fit the diagram onto the screen or to focus in on a portion of the diagram. You have the option of auto-arranging, zooming in, zooming out, or fitting the diagram to the screen. Click on the Zoom out (-) button and then Zoom in (+) The following is an example of a complex system where the diagram was fit to the screen. You can also use your mouse to click on part of the screen and \u201cmove\u201d it to center the diagram. To reduce the objects on the screen, you can filter them by object type using the filter icon: Filter by Catalogs to view the three that exist in the system To reset the screen back to the full diagram, choose reset in the filter dialog. Reset the Filters You can view the Infrastructure diagram as table by selecting the list view icon at the top right of the screen. Select List View Selecting the list icon will display the system as a table with a separate tab for Engines, Catalogs, Buckets, and Databases. Click on each tab in the table view to see the contents Reset back to the topology view. Switch back to topology view to the show the entire diagram","title":"Infrastructure Manager UI"},{"location":"wxd-infrastructure/#exploring-the-contents-of-a-catalog-or-bucket","text":"Hover your mouse over the hive_data catalog As you hover your mouse over the hive_data catalog in the topology view, the elements (engine, catalogs, buckets, databases) that are connected will be highlighted on the screen. To explore the contents of the hive_data catalog, click on the catalog icon. Click on the hive_data catalog icon Detailed information about the catalog is displayed on the screen. There are additional tabs at the top of the screen that provide information on the data objects that are cataloged and the access control for those objects. Click on the Data Objects tab The Data Objects tab provides detailed information on the contents of the catalog. This provides an easy way to explore the contents of the catalog. Click on the Access Control Tab To close this dialog, press the [x] in the top right corner of the list. Close the dialog by pressing the [x] in the corner You should now see the topology view of the system.","title":"Exploring the Contents of a Catalog or Bucket"},{"location":"wxd-infrastructure/#adding-resources-to-watsonxdata","text":"To view how engines, buckets, and databases can be added, click on the Add component on the top right of the Infrastructure screen and select Create Engine. Select Add component -> Database A dialog will appear that provides options for adding another bucket or database to the system. A production version of watsonx.data would also include an option for adding another compute engine to the system. The Database Type drop-down menu supports a number of databases. Select the database type drop-down The watsonx.data UI supports many types of databases that can be cataloged in the system. This list continues to expand with every release of the product. Press Cancel to return to the Infrastructure Screen","title":"Adding Resources to watsonx.data"},{"location":"wxd-infrastructure/#summary","text":"The Infrastructure view provides a way to visualize the topology of the watsonx.data system. The topology includes: Engines (blue layer) \u2013 The query engines that will access the data. Catalogs (purple layer) \u2013 Catalogs contain metadata about the objects found in buckets or databases. Buckets (green layer) \u2013 An object-store that contains data (i.e., MinIO, AWS S3, Cloud-Object Storage). Databases (blue layer) \u2013 Specialty Database engines. The Infrastructure view allows an administrator to add Engines, Catalogs, Buckets, and Databases to the system. In addition, the individual objects can be selected to view the definition, contents, and access control.","title":"Summary"},{"location":"wxd-intro/","text":"Introducing watsonx.data Watsonx.data is a core component of watsonx, IBM\u2019s enterprise-ready AI and data platform designed to multiply the impact of AI across an enterprise\u2019s business. The watsonx platform comprises three powerful components: the watsonx.ai studio for new foundation models, generative AI, and machine learning; the watsonx.data fit-for-purpose data store that provides the flexibility of a data lake with the performance of a data warehouse; plus, the watsonx.governance toolkit, to enable AI workflows that are built with responsibility, transparency, and explainability. The watsonx.data component (the focus of this lab) makes it possible for enterprises to scale analytics and AI with a data store built on an open lakehouse architecture, supported by querying, governance, and open data and table formats, to access and share data. With watsonx.data, enterprises can connect to data in minutes, quickly get trusted insights, and reduce their data warehouse costs. The next-gen watsonx.data lakehouse is designed to overcome the costs and complexities enterprises face. This will be the world\u2019s first and only open data store with multi-engine support that is built for hybrid deployment across your entire ecosystem. Watsonx.data is the only lakehouse with multiple query engines allowing you to optimize costs and performance by pairing the right workload with the right engine. Run all workloads from a single pane of glass, eliminating trade-offs with convenience while still improving cost and performance. Deploy anywhere with full support for hybrid-cloud and multi cloud environments. Shared metadata across multiple engines eliminates the need to re-catalog, accelerating time to value while ensuring governance and eliminating costly implementation efforts. This lab uses the watsonx.data developer package. The Developer package is meant to be used on single nodes. While it uses the same code base, there are some restrictions, especially on scale.","title":"Introducing watsonx.data"},{"location":"wxd-intro/#introducing-watsonxdata","text":"Watsonx.data is a core component of watsonx, IBM\u2019s enterprise-ready AI and data platform designed to multiply the impact of AI across an enterprise\u2019s business. The watsonx platform comprises three powerful components: the watsonx.ai studio for new foundation models, generative AI, and machine learning; the watsonx.data fit-for-purpose data store that provides the flexibility of a data lake with the performance of a data warehouse; plus, the watsonx.governance toolkit, to enable AI workflows that are built with responsibility, transparency, and explainability. The watsonx.data component (the focus of this lab) makes it possible for enterprises to scale analytics and AI with a data store built on an open lakehouse architecture, supported by querying, governance, and open data and table formats, to access and share data. With watsonx.data, enterprises can connect to data in minutes, quickly get trusted insights, and reduce their data warehouse costs. The next-gen watsonx.data lakehouse is designed to overcome the costs and complexities enterprises face. This will be the world\u2019s first and only open data store with multi-engine support that is built for hybrid deployment across your entire ecosystem. Watsonx.data is the only lakehouse with multiple query engines allowing you to optimize costs and performance by pairing the right workload with the right engine. Run all workloads from a single pane of glass, eliminating trade-offs with convenience while still improving cost and performance. Deploy anywhere with full support for hybrid-cloud and multi cloud environments. Shared metadata across multiple engines eliminates the need to re-catalog, accelerating time to value while ensuring governance and eliminating costly implementation efforts. This lab uses the watsonx.data developer package. The Developer package is meant to be used on single nodes. While it uses the same code base, there are some restrictions, especially on scale.","title":"Introducing watsonx.data"},{"location":"wxd-jupyter/","text":"Jupyter Notebook The watsonx.data server includes the Jupyter Notebook service which provides an interactive way of exploring the features of the Presto database. The link to the Jupyter Notebook table of contents is provided in your TechZone reservation. Find the URL in your reservation that reads Jupyter Notebook - Server: http://useast.services.cloud.techzone.ibm.com:xxxx/notebooks/Table_of_Contents.ipynb and click on it The initial screen will request that you enter a password. Enter watsonx.data as the password Once you have authenticated, the main table of contents will be displayed. There are 11 notebooks provided in the system, and a brief description of the notebooks are found below. Introduction to Jupyter Notebooks If you are not familiar with the use of Jupyter notebooks, this will be a good starting point. To view the notebook, click on the blue arrow found at the bottom of the box. This will open a new tab in your browser with the contents of the notebook. This notebook provides an introduction to what Jupyter Notebooks are and what the common tasks are that you can perform in a notebook. Watsonx.data Credentials This is a key notebook for you to use during your work with the watsonx.data system. This notebook provides details on the userids and passwords for the services that are running in the server. There is no need to use a terminal command line to determine what the credentials are! In addition to the userids and passwords, this notebook provides a convenient way of downloading the certificate required to connect to the Presto database. Simply click on the certificate link and it will be downloaded to your local machine. Presto Magic Commands Magic commands are special macros found in Jupyter notebooks that simplify many tasks, including the ability to run SQL commands against a database. This notebook provides an introduction to what magic commands are and how you can use the Presto magic commands to connect and query the Presto database. Introduction to Presto SQL The watsonx.data lab has two ways of running SQL against the Presto database: Presto CLI commands Python/Pandas/Magic commands This notebook contains all the SQL that is run in the Presto SQL section of the lab. Instead of using the presto-cli command, this notebook uses magic commands to simplify the SQL execution. You can choose either method to explore Presto SQL. Presto Federation Presto provides the ability to federate queries across different servers. This notebook explores the ability to federate a PostgreSQL table with a table found in Presto. This lab requires some knowledge of the watsonx.data UI, so it is recommended you become familiar with the UI before running this lab. Python with watsonx.data Accessing the Presto database in Python requires the use of the prestodb module which implements features of the DBAPI standard. The notebook demonstrates how to connect to the database and retrieve results. Pandas Dataframes with watsonx.data Pandas dataframes are commonly used in Jupyter notebooks to analyze data. This code will connect to Presto using a Pandas dataframe and display some data from an existing table that was created in Presto. Note that the certificate required for this notebook is provided in the environment. Accessing watsonx.data with Spark This notebook demonstrates how Spark can connect to watsonx.data and manipulate the data. This system has a local, minimally configured Spark engine that will be used to access the Presto database. This engine is sufficient to demonstrate the steps needed to connect to watsonx.data and access the data that resides in the Presto catalogs. Connecting to Db2 This notebook demonstrates connecting to the local Db2 server using Jupyter notebooks. Connecting to PostgreSQL This notebook demonstrates connecting to the local PostgreSQL server using Jupyter notebooks. Connecting to MySQL This notebook demonstrates connecting to the local MySQL server using Jupyter notebooks.","title":"Jupyter Notebook"},{"location":"wxd-jupyter/#jupyter-notebook","text":"The watsonx.data server includes the Jupyter Notebook service which provides an interactive way of exploring the features of the Presto database. The link to the Jupyter Notebook table of contents is provided in your TechZone reservation. Find the URL in your reservation that reads Jupyter Notebook - Server: http://useast.services.cloud.techzone.ibm.com:xxxx/notebooks/Table_of_Contents.ipynb and click on it The initial screen will request that you enter a password. Enter watsonx.data as the password Once you have authenticated, the main table of contents will be displayed. There are 11 notebooks provided in the system, and a brief description of the notebooks are found below.","title":"Jupyter Notebook"},{"location":"wxd-jupyter/#introduction-to-jupyter-notebooks","text":"If you are not familiar with the use of Jupyter notebooks, this will be a good starting point. To view the notebook, click on the blue arrow found at the bottom of the box. This will open a new tab in your browser with the contents of the notebook. This notebook provides an introduction to what Jupyter Notebooks are and what the common tasks are that you can perform in a notebook.","title":"Introduction to Jupyter Notebooks"},{"location":"wxd-jupyter/#watsonxdata-credentials","text":"This is a key notebook for you to use during your work with the watsonx.data system. This notebook provides details on the userids and passwords for the services that are running in the server. There is no need to use a terminal command line to determine what the credentials are! In addition to the userids and passwords, this notebook provides a convenient way of downloading the certificate required to connect to the Presto database. Simply click on the certificate link and it will be downloaded to your local machine.","title":"Watsonx.data Credentials"},{"location":"wxd-jupyter/#presto-magic-commands","text":"Magic commands are special macros found in Jupyter notebooks that simplify many tasks, including the ability to run SQL commands against a database. This notebook provides an introduction to what magic commands are and how you can use the Presto magic commands to connect and query the Presto database.","title":"Presto Magic Commands"},{"location":"wxd-jupyter/#introduction-to-presto-sql","text":"The watsonx.data lab has two ways of running SQL against the Presto database: Presto CLI commands Python/Pandas/Magic commands This notebook contains all the SQL that is run in the Presto SQL section of the lab. Instead of using the presto-cli command, this notebook uses magic commands to simplify the SQL execution. You can choose either method to explore Presto SQL.","title":"Introduction to Presto SQL"},{"location":"wxd-jupyter/#presto-federation","text":"Presto provides the ability to federate queries across different servers. This notebook explores the ability to federate a PostgreSQL table with a table found in Presto. This lab requires some knowledge of the watsonx.data UI, so it is recommended you become familiar with the UI before running this lab.","title":"Presto Federation"},{"location":"wxd-jupyter/#python-with-watsonxdata","text":"Accessing the Presto database in Python requires the use of the prestodb module which implements features of the DBAPI standard. The notebook demonstrates how to connect to the database and retrieve results.","title":"Python with watsonx.data"},{"location":"wxd-jupyter/#pandas-dataframes-with-watsonxdata","text":"Pandas dataframes are commonly used in Jupyter notebooks to analyze data. This code will connect to Presto using a Pandas dataframe and display some data from an existing table that was created in Presto. Note that the certificate required for this notebook is provided in the environment.","title":"Pandas Dataframes with watsonx.data"},{"location":"wxd-jupyter/#accessing-watsonxdata-with-spark","text":"This notebook demonstrates how Spark can connect to watsonx.data and manipulate the data. This system has a local, minimally configured Spark engine that will be used to access the Presto database. This engine is sufficient to demonstrate the steps needed to connect to watsonx.data and access the data that resides in the Presto catalogs.","title":"Accessing watsonx.data with Spark"},{"location":"wxd-jupyter/#connecting-to-db2","text":"This notebook demonstrates connecting to the local Db2 server using Jupyter notebooks.","title":"Connecting to Db2"},{"location":"wxd-jupyter/#connecting-to-postgresql","text":"This notebook demonstrates connecting to the local PostgreSQL server using Jupyter notebooks.","title":"Connecting to PostgreSQL"},{"location":"wxd-jupyter/#connecting-to-mysql","text":"This notebook demonstrates connecting to the local MySQL server using Jupyter notebooks.","title":"Connecting to MySQL"},{"location":"wxd-lab-instructions/","text":"Lab Instructions Lab Overview This lab is organized into a number of sections that cover many of the highlights and key features of watsonx.data. Introduction to watsonx.data Watsonx.data UI Navigation Infrastructure Manager Data Manager Query Workspace Time Travel Federation SQL Examples Lab Instructions Lab instructions may contain two types of information: Screen (UI) interactions Text commands When an action is required, the text will include a box with the instructions. Select the Infrastructure Icon in the watsonx.data UI Any text that needs to be typed into the system will be outlined in grey box. Enter this text into the SQL window and Run the code SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is usually found on the far right-hand side of the command box. Use this to copy the text and paste it into your dialog or command window. You can also select the text and copy it that way. Once you have copied the text, paste the value into the appropriate dialog using the paste command or menu. Copy and Paste Some commands may span multiple lines, so make sure you copy everything in the box if you are not using the copy button Icon Reference There are certain menu icons that are referred to throughout the lab that have specific names: Hamburger menu \u2261 This icon is used to display menu items that a user would select from. Kebab menu \u22ee This icon is usually used to indicate that there are specific actions that can be performed against an object. Twisty \u25ba and \u25bc Used to expand and collapse lists. URL Conventions Your TechZone reservation contains a number of URLs for the services provided in the watsonx.data server. The URL will contain the name of the server and the corresponding port number for the service. Throughout the documentation, the server name will be referred to as region.techzone-server.com and port number is referred to as port . Where you see these URLs, replace them with the values found in your reservation.","title":"Lab Instructions"},{"location":"wxd-lab-instructions/#lab-instructions","text":"","title":"Lab Instructions"},{"location":"wxd-lab-instructions/#lab-overview","text":"This lab is organized into a number of sections that cover many of the highlights and key features of watsonx.data. Introduction to watsonx.data Watsonx.data UI Navigation Infrastructure Manager Data Manager Query Workspace Time Travel Federation SQL Examples","title":"Lab Overview"},{"location":"wxd-lab-instructions/#lab-instructions_1","text":"Lab instructions may contain two types of information: Screen (UI) interactions Text commands When an action is required, the text will include a box with the instructions. Select the Infrastructure Icon in the watsonx.data UI Any text that needs to be typed into the system will be outlined in grey box. Enter this text into the SQL window and Run the code SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is usually found on the far right-hand side of the command box. Use this to copy the text and paste it into your dialog or command window. You can also select the text and copy it that way. Once you have copied the text, paste the value into the appropriate dialog using the paste command or menu. Copy and Paste Some commands may span multiple lines, so make sure you copy everything in the box if you are not using the copy button","title":"Lab Instructions"},{"location":"wxd-lab-instructions/#icon-reference","text":"There are certain menu icons that are referred to throughout the lab that have specific names: Hamburger menu \u2261 This icon is used to display menu items that a user would select from. Kebab menu \u22ee This icon is usually used to indicate that there are specific actions that can be performed against an object. Twisty \u25ba and \u25bc Used to expand and collapse lists.","title":"Icon Reference"},{"location":"wxd-lab-instructions/#url-conventions","text":"Your TechZone reservation contains a number of URLs for the services provided in the watsonx.data server. The URL will contain the name of the server and the corresponding port number for the service. Throughout the documentation, the server name will be referred to as region.techzone-server.com and port number is referred to as port . Where you see these URLs, replace them with the values found in your reservation.","title":"URL Conventions"},{"location":"wxd-minio/","text":"Using the MinIO console UI MinIO is a high-performance, S3 compatible object store. Rather than connect to an external S3 object store, we are going to use MinIO locally to run with watsonx.data. To connect to MinIO, you will need to extract the MinIO credentials by querying the docker container. You must be the root user to issue these commands. export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') echo \"MinIO Userid : \" $LH_S3_ACCESS_KEY echo \"MinIO Password: \" $LH_S3_SECRET_KEY MinIO Userid : c4643026087cc21989eb5c12 MinIO Password: 93da45c5af87abd86c9dbc83 You can get all passwords for the system when you are logged in as the watsonx user by using the following command. cat /certs/passwords Your TechZone reservation will include the server name and port number to use when connecting to the MinIO. The default port number is 9001, while the server will be referred to as region.techzone-server.com . Replace these values with those found in your reservation. Open your browser and navigate to: Minio console - http://region.techzone-server.com:port Note : Firefox on OSX occasionally freezes when connecting to the MinIO console. The Safari browser is much more reliable. Login with object store credentials found above (These will be different for your system). You should see current buckets in MinIO. We are going to examine these buckets after we populate them with some data. Creating Schemas and Tables Not all catalogs support creation of schemas - as an example, the TPCH catalog is not writeable. We will use the iceberg_data catalog for this exercise. We will need to get some details before we continue. Make sure you are connected as the root user and are in the proper directory. cd /root/ibm-lh-dev/bin Login to the Presto CLI. ./presto-cli --catalog iceberg_data Create schema workshop in catalog iceberg_data . Note how we are using the iceberg-bucket bucket which you should have seen in the MinIO object browser. CREATE SCHEMA IF NOT EXISTS workshop with (location='s3a://iceberg-bucket/'); Show the schemas available. show schemas; Schema ---------- workshop (1 row) Use the workshop schema. use workshop; Creating tables Create a new Apache Iceberg table using existing data in the sample Customer table as part of the TPCH catalog schema called TINY. create table customer as select * from tpch.tiny.customer; Show the tables. show tables; Table ---------- customer (1 row) Quit Presto. quit; \u2003 Refresh the Minio screen (see button on the far-right side). You should now see new objects under iceberg-bucket Click on the bucket name and you will see the customer table. Selecting the customer object will show that there is data and metadata in there. How do we know that this data is based on Apache iceberg? If you open the file under metadata , you should see metadata information for the data we are storing in parquet file format. Do I really need Apache Iceberg? YES, YOU DO! However, it is good to understand why? Metadata is also stored in the Parquet file format but only for the single parquet file. If we add more data/partitions, the data is split into multiple Parquet files, and we don\u2019t have a mechanism to get the table to parquet files mapping. Run the following example to understand this better. You need to get the access keys for MinIO before running the following lab. Make sure you are still connected as root . export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') Open the developer sandbox to connect to MinIO, download the selected parquet file and inspect the parquet file contents. ./dev-sandbox.sh Update the Python files to be executable (makes our commands more convenient). chmod +x /scripts/*.py List all files in the object store (MinIO). /scripts/s3-inspect.py --host ibm-lh-minio-svc:9000 --accessKey $LH_S3_ACCESS_KEY --secretKey $LH_S3_SECRET_KEY --bucket iceberg-bucket iceberg-bucket b'customer/data/e9536a5e-14a1-4823-98ed-cc22d6fc38db.parquet' 2023-06-06 14:31:47.778000+00:00 6737d7268fcb3eb459b675f27f716f48 75373 None iceberg-bucket b'customer/metadata/00000-e26c56e0-c4d7-4625-8b06-422429f6ba8d.metadata.json' 2023-06-06 14:31:48.629000+00:00 2e722c7dd83c1dd260a7e6c9503c0e04 3272 None iceberg-bucket b'customer/metadata/7cb074a4-3da7-4184-9db8-567383bb588a-m0.avro' 2023-06-06 14:31:48.401000+00:00 655a5568207cc399b8297f1488ef77e7 6342 None iceberg-bucket b'customer/metadata/snap-6143645832277262458-1-7cb074a4-3da7-4184-9db8-567383bb588a.avro' 2023-06-06 14:31:48.445000+00:00 0c3714299d43ae86a46eabdcaac1351e 3753 None You can extract the string with the following command. PARQUET=$(/scripts/s3-inspect.py --host ibm-lh-minio-svc:9000 --accessKey $LH_S3_ACCESS_KEY --secretKey $LH_S3_SECRET_KEY --bucket iceberg-bucket | grep -o -m 1 \".*'customer.*parquet\" | sed -n \"s/.*b'//p\") The file name that is retrieved is substituted into the next command. Note: The file name found in $PARQUET will be different on your system. /scripts/s3-download.py --host ibm-lh-minio-svc:9000 --accessKey $LH_S3_ACCESS_KEY --secretKey $LH_S3_SECRET_KEY --bucket iceberg-bucket --srcFile $PARQUET --destFile /tmp/x.parquet \u2003 Describe the File Contents. /scripts/describe-parquet.py /tmp/x.parquet ---------------------- metadata: created_by: num_columns: 8 num_rows: 1500 num_row_groups: 1 format_version: 1.0 serialized_size: 851 ---------------------- ---------------------- schema: custkey: int64 name: binary address: binary nationkey: int64 phone: binary acctbal: double mktsegment: binary comment: binary ---------------------- ---------------------- row group 0: num_columns: 8 num_rows: 1500 total_byte_size: 74555 ---------------------- ---------------------- row group 0, column 1: file_offset: 0 file_path: physical_type: BYTE_ARRAY num_values: 1500 path_in_schema: name is_stats_set: True statistics: has_min_max: False min: None max: None null_count: 0 distinct_count: 0 num_values: 1500 physical_type: BYTE_ARRAY logical_type: None converted_type (legacy): NONE compression: GZIP encodings: ('DELTA_BYTE_ARRAY',) has_dictionary_page: False dictionary_page_offset: None data_page_offset: 112 total_compressed_size: 599 total_uncompressed_size: 2806 ---------------------- Note : In this instance we used an insert into select * from customer with no partitioning defined there was only 1 parquet file and only 1 row group. This is not the norm, and we deliberately did this to show you the value of using Apache Iceberg file format which can be used by multiple runtimes to access Iceberg data stored in parquet format and managed by hive metastore. Exit from the Sandbox. exit MinIO CLI The MinIO Client mc command line tool provides an alternative to UNIX commands like ls , cat , cp , mirror , and diff with support for both file systems and Amazon S3-compatible cloud storage services. The mc commandline tool is built for compatibility with the AWS S3 API and is tested with MinIO and AWS S3 for expected functionality and behavior. Complete details and restrictions around the use of the CLI command can be found on the MinIO Client page. You can use the MinIO CLI from a variety of clients. The MinIO ports are open in the developer edition image, which provides an alternative to loading data directly from your workstation rather than using the MinIO UI interface. Minio System Alias Before running commands against the MinIO server, an alias must be created that includes the access and secret key. The values can be extracted from the system by listing the contents of the /certs/passwords file or by running the passwords command as the root user. cat /certs/passwords The values for the MinIO access and secret key can also be exported with the following code: export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') echo \"MinIO Userid : \" $LH_S3_ACCESS_KEY echo \"MinIO Password: \" $LH_S3_SECRET_KEY The alias command has the following syntax: mc alias set alias-name hostname:port access_key secret_key For a local connection, we will use the following values: Alias Name - watsonxdata Hostname \u2013 watsonxdata Port \u2013 9000 Access Key \u2013 $LH_S3_ACCESS_KEY Secret Key - $LH_S3_SECRET_KEY If you are using an external client to connect to the MinIO service, you will need the URL and Port number from the TechZone reservation. The access key and secret key will be the same values that are found above. Hostname \u2013 region.techzone-server.com Port \u2013 12345 The alias for local access is found below. mc alias set watsonxdata http://watsonxdata:9000 $LH_S3_ACCESS_KEY $LH_S3_SECRET_KEY Added `watsonxdata` successfully. List Buckets The mc command provides us with a number of commands that allows us to manage buckets and files within them. The following command checks to see what buckets currently exist in the system. mc ls tree watsonxdata [2023-09-29 14:38:19 EDT] 0B hive-bucket/ [2023-09-29 14:38:19 EDT] 0B iceberg-bucket/ You can view the contents of a bucket by traversing down the path. mc ls tree watsonxdata/hive-bucket [2023-10-13 10:34:36 EDT] 0B gosalesdw/ [2023-10-13 10:34:36 EDT] 0B hive_sql/ [2023-10-13 10:34:36 EDT] 0B ontime/ [2023-10-13 10:34:36 EDT] 0B taxi/ Create a Bucket At this point we will create a new bucket to hold some data. Use the mb (make bucket) command. The command requires the alias name for the MinIO connection followed by the name of the bucket. mc mb alias-name/new-bucket The following code will create a new bucket in the system called sampledata . mc mb watsonxdata/sampledata Bucket created successfully `watsonxdata/sampledata`. We can double check that the bucket it there. mc ls tree watsonxdata [2023-09-29 14:38:19 EDT] 0B hive-bucket/ [2023-09-29 14:38:19 EDT] 0B iceberg-bucket/ [2023-10-13 10:39:47 EDT] 0B sampledata/ Loading Data One of the most powerful features of the MinIO CLI is its ability to load data directory from your workstation into the bucket, rather than having to use the MinIO UI. It is also significantly faster than using the UI interface. The next example will load data into the bucket that was just created. The directory that we will be using to load data from is called /sampledata and found in the root directory of the watsonx.data server. ls /sampledata/csv gosales ontime taxi Next we will load the data from each one of these directories into the sampledata bucket. The mc command allows you to select which files to place into a bucket, or an entire directory with recursion. In this case we are loading all three directories the files into the bucket. Note the use of the / at the end of the directory name to prevent the directory name csv from being used as the high-level directory name in the target bucket. mc cp --recursive /sampledata/csv/ watsonxdata/sampledata/ ...data/csv/taxi/taxi.csv: 306.16 MiB / 306.16 MiB \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 147.91 MiB/s 2s We can double-check that our files are there with the --files option: mc tree --files watsonxdata/sampledata/ watsonxdata/sampledata/ \u251c\u2500 gosales \u2502 \u251c\u2500 DIST_INVENTORY_FACT.csv \u2502 \u251c\u2500 DIST_PRODUCT_FORECAST_FACT.csv \u2502 \u251c\u2500 DIST_RETURNED_ITEMS_FACT.csv \u2502 \u251c\u2500 DIST_RETURN_REASON_DIM.csv .... \u2502 \u251c\u2500 EMP_EMPLOYEE_DIM.csv \u2502 \u251c\u2500 SLS_SALES_TARG_FACT.csv \u2502 \u251c\u2500 gosales_createtable.sql \u2502 \u2514\u2500 gosales_load_postgres.sql \u251c\u2500 ontime \u2502 \u251c\u2500 aircraft.csv \u2502 \u251c\u2500 airline_id.csv \u2502 \u251c\u2500 airport_id.csv \u2502 \u251c\u2500 cancellation.csv \u2502 \u2514\u2500 ontime.csv \u2514\u2500 taxi \u2514\u2500 taxi.csv Delete a File or Bucket Use the rb (Remove bucket) command to remove a bucket and its contents. You can remove individual objects by using the rm (Remove) command by fully qualifying the object. The next command will remove the ontime.csv file from the ontime folder. mc rm watsonxdata/sampledata/ontime/ontime.csv Removed `watsonxdata/sampledata/ontime/ontime.csv`. The delete bucket command will fail if you still have data in the bucket. mc rb watsonxdata/sampledata mc: `watsonxdata/sampledata` is not empty. Retry this command with \u2018--force\u2019 flag if you want to remove `watsonxdata/sampledata` and all its contents Adding the --force option will remove the bucket and all the data in it. Use with caution! mc rb --force watsonxdata/sampledata Removed `watsonxdata/sampledata` successfully.","title":"Using the MinIO console UI"},{"location":"wxd-minio/#using-the-minio-console-ui","text":"MinIO is a high-performance, S3 compatible object store. Rather than connect to an external S3 object store, we are going to use MinIO locally to run with watsonx.data. To connect to MinIO, you will need to extract the MinIO credentials by querying the docker container. You must be the root user to issue these commands. export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') echo \"MinIO Userid : \" $LH_S3_ACCESS_KEY echo \"MinIO Password: \" $LH_S3_SECRET_KEY MinIO Userid : c4643026087cc21989eb5c12 MinIO Password: 93da45c5af87abd86c9dbc83 You can get all passwords for the system when you are logged in as the watsonx user by using the following command. cat /certs/passwords Your TechZone reservation will include the server name and port number to use when connecting to the MinIO. The default port number is 9001, while the server will be referred to as region.techzone-server.com . Replace these values with those found in your reservation. Open your browser and navigate to: Minio console - http://region.techzone-server.com:port Note : Firefox on OSX occasionally freezes when connecting to the MinIO console. The Safari browser is much more reliable. Login with object store credentials found above (These will be different for your system). You should see current buckets in MinIO. We are going to examine these buckets after we populate them with some data.","title":"Using the MinIO console UI"},{"location":"wxd-minio/#creating-schemas-and-tables","text":"Not all catalogs support creation of schemas - as an example, the TPCH catalog is not writeable. We will use the iceberg_data catalog for this exercise. We will need to get some details before we continue. Make sure you are connected as the root user and are in the proper directory. cd /root/ibm-lh-dev/bin Login to the Presto CLI. ./presto-cli --catalog iceberg_data Create schema workshop in catalog iceberg_data . Note how we are using the iceberg-bucket bucket which you should have seen in the MinIO object browser. CREATE SCHEMA IF NOT EXISTS workshop with (location='s3a://iceberg-bucket/'); Show the schemas available. show schemas; Schema ---------- workshop (1 row) Use the workshop schema. use workshop;","title":"Creating Schemas and Tables"},{"location":"wxd-minio/#creating-tables","text":"Create a new Apache Iceberg table using existing data in the sample Customer table as part of the TPCH catalog schema called TINY. create table customer as select * from tpch.tiny.customer; Show the tables. show tables; Table ---------- customer (1 row) Quit Presto. quit; \u2003 Refresh the Minio screen (see button on the far-right side). You should now see new objects under iceberg-bucket Click on the bucket name and you will see the customer table. Selecting the customer object will show that there is data and metadata in there. How do we know that this data is based on Apache iceberg? If you open the file under metadata , you should see metadata information for the data we are storing in parquet file format.","title":"Creating tables"},{"location":"wxd-minio/#do-i-really-need-apache-iceberg","text":"YES, YOU DO! However, it is good to understand why? Metadata is also stored in the Parquet file format but only for the single parquet file. If we add more data/partitions, the data is split into multiple Parquet files, and we don\u2019t have a mechanism to get the table to parquet files mapping. Run the following example to understand this better. You need to get the access keys for MinIO before running the following lab. Make sure you are still connected as root . export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') Open the developer sandbox to connect to MinIO, download the selected parquet file and inspect the parquet file contents. ./dev-sandbox.sh Update the Python files to be executable (makes our commands more convenient). chmod +x /scripts/*.py List all files in the object store (MinIO). /scripts/s3-inspect.py --host ibm-lh-minio-svc:9000 --accessKey $LH_S3_ACCESS_KEY --secretKey $LH_S3_SECRET_KEY --bucket iceberg-bucket iceberg-bucket b'customer/data/e9536a5e-14a1-4823-98ed-cc22d6fc38db.parquet' 2023-06-06 14:31:47.778000+00:00 6737d7268fcb3eb459b675f27f716f48 75373 None iceberg-bucket b'customer/metadata/00000-e26c56e0-c4d7-4625-8b06-422429f6ba8d.metadata.json' 2023-06-06 14:31:48.629000+00:00 2e722c7dd83c1dd260a7e6c9503c0e04 3272 None iceberg-bucket b'customer/metadata/7cb074a4-3da7-4184-9db8-567383bb588a-m0.avro' 2023-06-06 14:31:48.401000+00:00 655a5568207cc399b8297f1488ef77e7 6342 None iceberg-bucket b'customer/metadata/snap-6143645832277262458-1-7cb074a4-3da7-4184-9db8-567383bb588a.avro' 2023-06-06 14:31:48.445000+00:00 0c3714299d43ae86a46eabdcaac1351e 3753 None You can extract the string with the following command. PARQUET=$(/scripts/s3-inspect.py --host ibm-lh-minio-svc:9000 --accessKey $LH_S3_ACCESS_KEY --secretKey $LH_S3_SECRET_KEY --bucket iceberg-bucket | grep -o -m 1 \".*'customer.*parquet\" | sed -n \"s/.*b'//p\") The file name that is retrieved is substituted into the next command. Note: The file name found in $PARQUET will be different on your system. /scripts/s3-download.py --host ibm-lh-minio-svc:9000 --accessKey $LH_S3_ACCESS_KEY --secretKey $LH_S3_SECRET_KEY --bucket iceberg-bucket --srcFile $PARQUET --destFile /tmp/x.parquet \u2003 Describe the File Contents. /scripts/describe-parquet.py /tmp/x.parquet ---------------------- metadata: created_by: num_columns: 8 num_rows: 1500 num_row_groups: 1 format_version: 1.0 serialized_size: 851 ---------------------- ---------------------- schema: custkey: int64 name: binary address: binary nationkey: int64 phone: binary acctbal: double mktsegment: binary comment: binary ---------------------- ---------------------- row group 0: num_columns: 8 num_rows: 1500 total_byte_size: 74555 ---------------------- ---------------------- row group 0, column 1: file_offset: 0 file_path: physical_type: BYTE_ARRAY num_values: 1500 path_in_schema: name is_stats_set: True statistics: has_min_max: False min: None max: None null_count: 0 distinct_count: 0 num_values: 1500 physical_type: BYTE_ARRAY logical_type: None converted_type (legacy): NONE compression: GZIP encodings: ('DELTA_BYTE_ARRAY',) has_dictionary_page: False dictionary_page_offset: None data_page_offset: 112 total_compressed_size: 599 total_uncompressed_size: 2806 ---------------------- Note : In this instance we used an insert into select * from customer with no partitioning defined there was only 1 parquet file and only 1 row group. This is not the norm, and we deliberately did this to show you the value of using Apache Iceberg file format which can be used by multiple runtimes to access Iceberg data stored in parquet format and managed by hive metastore. Exit from the Sandbox. exit","title":"Do I really need Apache Iceberg?"},{"location":"wxd-minio/#minio-cli","text":"The MinIO Client mc command line tool provides an alternative to UNIX commands like ls , cat , cp , mirror , and diff with support for both file systems and Amazon S3-compatible cloud storage services. The mc commandline tool is built for compatibility with the AWS S3 API and is tested with MinIO and AWS S3 for expected functionality and behavior. Complete details and restrictions around the use of the CLI command can be found on the MinIO Client page. You can use the MinIO CLI from a variety of clients. The MinIO ports are open in the developer edition image, which provides an alternative to loading data directly from your workstation rather than using the MinIO UI interface.","title":"MinIO CLI"},{"location":"wxd-minio/#minio-system-alias","text":"Before running commands against the MinIO server, an alias must be created that includes the access and secret key. The values can be extracted from the system by listing the contents of the /certs/passwords file or by running the passwords command as the root user. cat /certs/passwords The values for the MinIO access and secret key can also be exported with the following code: export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') echo \"MinIO Userid : \" $LH_S3_ACCESS_KEY echo \"MinIO Password: \" $LH_S3_SECRET_KEY The alias command has the following syntax: mc alias set alias-name hostname:port access_key secret_key For a local connection, we will use the following values: Alias Name - watsonxdata Hostname \u2013 watsonxdata Port \u2013 9000 Access Key \u2013 $LH_S3_ACCESS_KEY Secret Key - $LH_S3_SECRET_KEY If you are using an external client to connect to the MinIO service, you will need the URL and Port number from the TechZone reservation. The access key and secret key will be the same values that are found above. Hostname \u2013 region.techzone-server.com Port \u2013 12345 The alias for local access is found below. mc alias set watsonxdata http://watsonxdata:9000 $LH_S3_ACCESS_KEY $LH_S3_SECRET_KEY Added `watsonxdata` successfully.","title":"Minio System Alias"},{"location":"wxd-minio/#list-buckets","text":"The mc command provides us with a number of commands that allows us to manage buckets and files within them. The following command checks to see what buckets currently exist in the system. mc ls tree watsonxdata [2023-09-29 14:38:19 EDT] 0B hive-bucket/ [2023-09-29 14:38:19 EDT] 0B iceberg-bucket/ You can view the contents of a bucket by traversing down the path. mc ls tree watsonxdata/hive-bucket [2023-10-13 10:34:36 EDT] 0B gosalesdw/ [2023-10-13 10:34:36 EDT] 0B hive_sql/ [2023-10-13 10:34:36 EDT] 0B ontime/ [2023-10-13 10:34:36 EDT] 0B taxi/","title":"List Buckets"},{"location":"wxd-minio/#create-a-bucket","text":"At this point we will create a new bucket to hold some data. Use the mb (make bucket) command. The command requires the alias name for the MinIO connection followed by the name of the bucket. mc mb alias-name/new-bucket The following code will create a new bucket in the system called sampledata . mc mb watsonxdata/sampledata Bucket created successfully `watsonxdata/sampledata`. We can double check that the bucket it there. mc ls tree watsonxdata [2023-09-29 14:38:19 EDT] 0B hive-bucket/ [2023-09-29 14:38:19 EDT] 0B iceberg-bucket/ [2023-10-13 10:39:47 EDT] 0B sampledata/","title":"Create a Bucket"},{"location":"wxd-minio/#loading-data","text":"One of the most powerful features of the MinIO CLI is its ability to load data directory from your workstation into the bucket, rather than having to use the MinIO UI. It is also significantly faster than using the UI interface. The next example will load data into the bucket that was just created. The directory that we will be using to load data from is called /sampledata and found in the root directory of the watsonx.data server. ls /sampledata/csv gosales ontime taxi Next we will load the data from each one of these directories into the sampledata bucket. The mc command allows you to select which files to place into a bucket, or an entire directory with recursion. In this case we are loading all three directories the files into the bucket. Note the use of the / at the end of the directory name to prevent the directory name csv from being used as the high-level directory name in the target bucket. mc cp --recursive /sampledata/csv/ watsonxdata/sampledata/ ...data/csv/taxi/taxi.csv: 306.16 MiB / 306.16 MiB \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 147.91 MiB/s 2s We can double-check that our files are there with the --files option: mc tree --files watsonxdata/sampledata/ watsonxdata/sampledata/ \u251c\u2500 gosales \u2502 \u251c\u2500 DIST_INVENTORY_FACT.csv \u2502 \u251c\u2500 DIST_PRODUCT_FORECAST_FACT.csv \u2502 \u251c\u2500 DIST_RETURNED_ITEMS_FACT.csv \u2502 \u251c\u2500 DIST_RETURN_REASON_DIM.csv .... \u2502 \u251c\u2500 EMP_EMPLOYEE_DIM.csv \u2502 \u251c\u2500 SLS_SALES_TARG_FACT.csv \u2502 \u251c\u2500 gosales_createtable.sql \u2502 \u2514\u2500 gosales_load_postgres.sql \u251c\u2500 ontime \u2502 \u251c\u2500 aircraft.csv \u2502 \u251c\u2500 airline_id.csv \u2502 \u251c\u2500 airport_id.csv \u2502 \u251c\u2500 cancellation.csv \u2502 \u2514\u2500 ontime.csv \u2514\u2500 taxi \u2514\u2500 taxi.csv","title":"Loading Data"},{"location":"wxd-minio/#delete-a-file-or-bucket","text":"Use the rb (Remove bucket) command to remove a bucket and its contents. You can remove individual objects by using the rm (Remove) command by fully qualifying the object. The next command will remove the ontime.csv file from the ontime folder. mc rm watsonxdata/sampledata/ontime/ontime.csv Removed `watsonxdata/sampledata/ontime/ontime.csv`. The delete bucket command will fail if you still have data in the bucket. mc rb watsonxdata/sampledata mc: `watsonxdata/sampledata` is not empty. Retry this command with \u2018--force\u2019 flag if you want to remove `watsonxdata/sampledata` and all its contents Adding the --force option will remove the bucket and all the data in it. Use with caution! mc rb --force watsonxdata/sampledata Removed `watsonxdata/sampledata` successfully.","title":"Delete a File or Bucket"},{"location":"wxd-objectstore/","text":"Working with Object Store Buckets In this lab, we will run through some exercises to understand how the watsonx.data can be configured to work with multiple buckets, using IBM COS, in addition to the out of the box MinIO bucket. In the GA version, there will be a user experience to facilitate such setup, however this lab will help you understand some service-service interactions & configurations. Why do we need to do this? In this lab, we will use multiple buckets as this is also how we can illustrate compute-storage separation. Out of the box, both in SaaS and Software, a tiny Object Store bucket is allocated, primarily for getting started use cases. Customers would need to point to their own bucket for their data. The use of a remote bucket (in this example, MinIO) also showcases the \"open\" aspect of the watsonx.data system. Customers own their data and can physically access the iceberg-ed bucket using other applications or engines, even custom ones that they build themselves. Customers would also have requirements to place (data sovereignty) buckets in specific locations. Compute/analytics engines may need to run in different locations, say closer to applications and connect to buckets in other networks/geos. There will also be situations where the same engine federates data across multiple buckets (and other database connections). As part of the GA release, there will also be authorization & data access rules that will control which user/group can access buckets even within the same engine. In Enterprise/Production environments, engines are expected to be ephemeral or there can be multiple engines. These engines when they come up will connect to different object store buckets. The list of engines will include Db2, NZ, IBM Analytics Engine for Spark, apart from Presto. The shared meta-store is critical in all of this as it helps provide relevant schema information to the engines. Create new bucket in MinIO Open your browser and navigate to the MinIO console. Check to see if the MinIO credentials exist in your terminal session. printf \"\\nAccess Key: $LH_S3_ACCESS_KEY \\nSecret Key: $LH_S3_SECRET_KEY\\n\" Userid : fcf1ec270e05a5031ca27bc9 Password: a671febd9e1e3826cf8cdcf5 If these values are blank, you need to run the following command. export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') Click on the Buckets tab to show the current buckets in the MinIO system. You can see that we have two buckets used for the labs. We need to create a new bucket to use for our schema. Press the \"Create Bucket +\" option on the right side of the screen. Note : The size and contents of the existing buckets will be different on your system. Enter a bucket name (customer) and then press Create Bucket. You should now see your new bucket below. Open your browser and connect to the watsonx.data UI: Navigate to the Infrastructure manager by clicking on the icon below the Home symbol. Get the S3 bucket credentials. printf \"\\nAccess Key: $LH_S3_ACCESS_KEY \\nSecret Key: $LH_S3_SECRET_KEY\\n\" Click on the Add component menu and select Add bucket. Fill in the dialog with the following values. Bucket type \u2013 MinIO Bucket name \u2013 customer Display name \u2013 customer Endpoint \u2013 http://ibm-lh-minio-svc:9000 Access key \u2013 $LH_S3_ACCESS_KEY (contents of this value) Secret key \u2013 $LH_S3_SECRET_KEY (contents of this value) Activate now \u2013 Yes Catalog type - Apache Iceberg Catalog name - customer When done press Add and Activate now. Your UI should change to display the new bucket (Your screen may be slightly different). Note : This step may take a minute to complete. At this point you need to Associate the bucket with the Presto engine. When you hover your mouse over the Customer catalog and the Associate icon will display. If you do not see the Associate icon, refresh the browser page. Press the associate button and the following dialog will display. Select the presto-01 engine and then press the Save and restart engine button. Associate button and wait for the screen to refresh. Note : Your display will be different. Exploring the Customer bucket First check to make sure that the Presto engine has finished starting. While the watsonx.data UI has restarted the Presto process, it takes a few seconds to become available. check_presto Switch to the bin directory as the root user. sudo su - cd /root/ibm-lh-dev/bin Connect to Presto using the new customer catalog. ./presto-cli --catalog customer We will create a schema where we store our table data using the new catalog name we created for the customer bucket. CREATE SCHEMA IF NOT EXISTS newworkshop with (location='s3a://customer/'); Switch to the new schema. use newworkshop; Use the following SQL to create a new table in the customer bucket. create table customer as select * from tpch.tiny.customer; CREATE TABLE: 1500 rows Quit Presto. quit; You can use the Developer sandbox (bin/dev-sandbox.sh), as described in MinIO UI , to inspect the Customer bucket with the s3-inspect utility. It's easier to use the MinIO console to view the bucket instead. Open your browser and navigate to the MinIO console. From the main screen select Object Browser and view the contents of the customer bucket. Note : You can continue to add new buckets when working with the watsonx.data UI. However, if you delete the catalog or bucket in the UI, you may find that you may not be able to re-catalog it. If you find that this happens, create another bucket, or rename the original one if that is possible.","title":"Working with Object Store Buckets"},{"location":"wxd-objectstore/#working-with-object-store-buckets","text":"In this lab, we will run through some exercises to understand how the watsonx.data can be configured to work with multiple buckets, using IBM COS, in addition to the out of the box MinIO bucket. In the GA version, there will be a user experience to facilitate such setup, however this lab will help you understand some service-service interactions & configurations.","title":"Working with Object Store Buckets"},{"location":"wxd-objectstore/#why-do-we-need-to-do-this","text":"In this lab, we will use multiple buckets as this is also how we can illustrate compute-storage separation. Out of the box, both in SaaS and Software, a tiny Object Store bucket is allocated, primarily for getting started use cases. Customers would need to point to their own bucket for their data. The use of a remote bucket (in this example, MinIO) also showcases the \"open\" aspect of the watsonx.data system. Customers own their data and can physically access the iceberg-ed bucket using other applications or engines, even custom ones that they build themselves. Customers would also have requirements to place (data sovereignty) buckets in specific locations. Compute/analytics engines may need to run in different locations, say closer to applications and connect to buckets in other networks/geos. There will also be situations where the same engine federates data across multiple buckets (and other database connections). As part of the GA release, there will also be authorization & data access rules that will control which user/group can access buckets even within the same engine. In Enterprise/Production environments, engines are expected to be ephemeral or there can be multiple engines. These engines when they come up will connect to different object store buckets. The list of engines will include Db2, NZ, IBM Analytics Engine for Spark, apart from Presto. The shared meta-store is critical in all of this as it helps provide relevant schema information to the engines.","title":"Why do we need to do this?"},{"location":"wxd-objectstore/#create-new-bucket-in-minio","text":"Open your browser and navigate to the MinIO console. Check to see if the MinIO credentials exist in your terminal session. printf \"\\nAccess Key: $LH_S3_ACCESS_KEY \\nSecret Key: $LH_S3_SECRET_KEY\\n\" Userid : fcf1ec270e05a5031ca27bc9 Password: a671febd9e1e3826cf8cdcf5 If these values are blank, you need to run the following command. export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') Click on the Buckets tab to show the current buckets in the MinIO system. You can see that we have two buckets used for the labs. We need to create a new bucket to use for our schema. Press the \"Create Bucket +\" option on the right side of the screen. Note : The size and contents of the existing buckets will be different on your system. Enter a bucket name (customer) and then press Create Bucket. You should now see your new bucket below. Open your browser and connect to the watsonx.data UI: Navigate to the Infrastructure manager by clicking on the icon below the Home symbol. Get the S3 bucket credentials. printf \"\\nAccess Key: $LH_S3_ACCESS_KEY \\nSecret Key: $LH_S3_SECRET_KEY\\n\" Click on the Add component menu and select Add bucket. Fill in the dialog with the following values. Bucket type \u2013 MinIO Bucket name \u2013 customer Display name \u2013 customer Endpoint \u2013 http://ibm-lh-minio-svc:9000 Access key \u2013 $LH_S3_ACCESS_KEY (contents of this value) Secret key \u2013 $LH_S3_SECRET_KEY (contents of this value) Activate now \u2013 Yes Catalog type - Apache Iceberg Catalog name - customer When done press Add and Activate now. Your UI should change to display the new bucket (Your screen may be slightly different). Note : This step may take a minute to complete. At this point you need to Associate the bucket with the Presto engine. When you hover your mouse over the Customer catalog and the Associate icon will display. If you do not see the Associate icon, refresh the browser page. Press the associate button and the following dialog will display. Select the presto-01 engine and then press the Save and restart engine button. Associate button and wait for the screen to refresh. Note : Your display will be different.","title":"Create new bucket in MinIO"},{"location":"wxd-objectstore/#exploring-the-customer-bucket","text":"First check to make sure that the Presto engine has finished starting. While the watsonx.data UI has restarted the Presto process, it takes a few seconds to become available. check_presto Switch to the bin directory as the root user. sudo su - cd /root/ibm-lh-dev/bin Connect to Presto using the new customer catalog. ./presto-cli --catalog customer We will create a schema where we store our table data using the new catalog name we created for the customer bucket. CREATE SCHEMA IF NOT EXISTS newworkshop with (location='s3a://customer/'); Switch to the new schema. use newworkshop; Use the following SQL to create a new table in the customer bucket. create table customer as select * from tpch.tiny.customer; CREATE TABLE: 1500 rows Quit Presto. quit; You can use the Developer sandbox (bin/dev-sandbox.sh), as described in MinIO UI , to inspect the Customer bucket with the s3-inspect utility. It's easier to use the MinIO console to view the bucket instead. Open your browser and navigate to the MinIO console. From the main screen select Object Browser and view the contents of the customer bucket. Note : You can continue to add new buckets when working with the watsonx.data UI. However, if you delete the catalog or bucket in the UI, you may find that you may not be able to re-catalog it. If you find that this happens, create another bucket, or rename the original one if that is possible.","title":"Exploring the Customer bucket"},{"location":"wxd-presto/","text":"Using the Presto console UI Your TechZone reservation will include the server name and port number to use when connecting to the Presto UI. Find the URL in your reservation that reads Presto console - https://useast.services.cloud.techzone.ibm.com:xxxx and click on it You will be presented with a logon screen. Enter the following credentials Username: ibmlhadmin Password: password The Presto console allows you to do the following: Monitor state of the cluster Queries being executed Queries in queue Data throughput Query details (text and plan) Note : The Presto console is very valuable when it comes to diagnosing problems with any queries you run in the watsonx.data environment. If a query fails you can find more details in the Presto console using the instructions below. On the main Presto screen, click the Finished Button (middle of the screen). A list of finished queries will display below the tab bar. You can scroll through the list of queries and get details of the execution plans. If you scroll through the list, you should see the test query \"select * from customer limit 5\". If you had a query that failed, look for the SQL in this list and continue on with the next step. Click on the query ID to see details of the execution plan that Presto produced. You can get more information about the query by clicking on any of the tabs that are on this screen. For instance, the Live Plan tab will show a visual explain of the stages that the query went through during execution. Scrolling to the bottom of this screen will also display any error messages that may have been produced by the SQL. Take time to check out the other information that is available for the query including the stage performance.","title":"Presto UI"},{"location":"wxd-presto/#using-the-presto-console-ui","text":"Your TechZone reservation will include the server name and port number to use when connecting to the Presto UI. Find the URL in your reservation that reads Presto console - https://useast.services.cloud.techzone.ibm.com:xxxx and click on it You will be presented with a logon screen. Enter the following credentials Username: ibmlhadmin Password: password The Presto console allows you to do the following: Monitor state of the cluster Queries being executed Queries in queue Data throughput Query details (text and plan) Note : The Presto console is very valuable when it comes to diagnosing problems with any queries you run in the watsonx.data environment. If a query fails you can find more details in the Presto console using the instructions below. On the main Presto screen, click the Finished Button (middle of the screen). A list of finished queries will display below the tab bar. You can scroll through the list of queries and get details of the execution plans. If you scroll through the list, you should see the test query \"select * from customer limit 5\". If you had a query that failed, look for the SQL in this list and continue on with the next step. Click on the query ID to see details of the execution plan that Presto produced. You can get more information about the query by clicking on any of the tabs that are on this screen. For instance, the Live Plan tab will show a visual explain of the stages that the query went through during execution. Scrolling to the bottom of this screen will also display any error messages that may have been produced by the SQL. Take time to check out the other information that is available for the query including the stage performance.","title":"Using the Presto console UI"},{"location":"wxd-prestocli/","text":"PrestoDB SQL Watsonx.data is based on open source PrestoDB, a distributed query engine that enables querying data stored in open file formats using open table formats for optimization or performance. Connectivity to watsonx.data can be done using the following methods: Command line interface (CLI) JDBC drivers watsonx.data UI This lab will be using the watsonx.data UI to issue the SQL commands. Please make sure to review the Using the Query Workspace lab before running the examples in this lab. The SQL statements that you will be executing will be displayed in this format: Sample SQL SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is found on the far right-hand side of the command box. Use this to copy the text and paste it the SQL window. You can also select the text and copy it that way. The expected results are displayed below the SQL command (your results may vary depending on the query). Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Catalog We are going to inspect the available catalogs in the watsonx.data system. A watsonx.data catalog contains schemas and references a data source via a connector. A connector is like a driver for a database. Watsonx.data connectors are an implementation of Presto\u2019s SPI which allows Presto to interact with a resource. There are several built-in connectors for JMX, Hive, TPCH etc., some of which you will use as part of the labs. Display the catalogs show catalogs; Let's look up what schemas are available with any given catalog. We will use the TPCH catalog which is an internal PrestoDB auto-generated catalog and look at the available schemas. Show schemas in tpch show schemas in tpch; You can connect to a specific catalog using the USE command. Rather than having to type out the catalog name, you can use this command to make the catalog the default for any SQL that doesn't include it. The command format is: USE catalog USE catalog.schema Set the catalog to tpch.tiny USE tpch.tiny; Look at the available tables in the TPCH catalog under the tiny schema. View tables in the current schema show tables; Your SQL will fail! The SQL doesn't quite work as expected! The reason is that each SQL statement executed in the SQL window has an independent context. What this means is that the catalog name is reset when you run another SQL statement after the USE statement is executed. If we bundle the two commands together, the SQL will work. Set the catalog to tpch.tiny and show the tables USE tpch.tiny; show tables; The describe command is used to display the format of a table. Inspect schema of the customer table use tpch.tiny; describe customer; You could also use the syntax below to achieve the same result. Show the customer table columns using an alternate format show columns from tpch.tiny.customer; The Presto engine includes a number of built-in functions. The following SQL will return a list of available Date functions. \u2003 Inspect available Date functions show functions like 'date%'; Now we will switch to a different schema and display the tables in it. In this example we use the show command with the IN clause to tell the system which catalog and schema to use. View the contents of the sf1 schema show tables in tpch.sf1; Query data from customer table select * from tpch.sf1.customer limit 5; Presto gathers statistics for tables in order to generate more accurate access plans. This SQL will gather statistics on a given table. Gather statistics on the customer table show stats for tpch.sf1.customer; Summary This lab has provided an introduction to some of the SQL that is available in Presto and watsonx.data. The next set of labs will demonstrate some more sophisticated SQL that can be run in the Presto engine.","title":"Presto SQL Overview"},{"location":"wxd-prestocli/#prestodb-sql","text":"Watsonx.data is based on open source PrestoDB, a distributed query engine that enables querying data stored in open file formats using open table formats for optimization or performance. Connectivity to watsonx.data can be done using the following methods: Command line interface (CLI) JDBC drivers watsonx.data UI This lab will be using the watsonx.data UI to issue the SQL commands. Please make sure to review the Using the Query Workspace lab before running the examples in this lab. The SQL statements that you will be executing will be displayed in this format: Sample SQL SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is found on the far right-hand side of the command box. Use this to copy the text and paste it the SQL window. You can also select the text and copy it that way. The expected results are displayed below the SQL command (your results may vary depending on the query). Before starting, make sure you are in the Query Workspace by clicking this icon on the left side","title":"PrestoDB SQL"},{"location":"wxd-prestocli/#catalog","text":"We are going to inspect the available catalogs in the watsonx.data system. A watsonx.data catalog contains schemas and references a data source via a connector. A connector is like a driver for a database. Watsonx.data connectors are an implementation of Presto\u2019s SPI which allows Presto to interact with a resource. There are several built-in connectors for JMX, Hive, TPCH etc., some of which you will use as part of the labs. Display the catalogs show catalogs; Let's look up what schemas are available with any given catalog. We will use the TPCH catalog which is an internal PrestoDB auto-generated catalog and look at the available schemas. Show schemas in tpch show schemas in tpch; You can connect to a specific catalog using the USE command. Rather than having to type out the catalog name, you can use this command to make the catalog the default for any SQL that doesn't include it. The command format is: USE catalog USE catalog.schema Set the catalog to tpch.tiny USE tpch.tiny; Look at the available tables in the TPCH catalog under the tiny schema. View tables in the current schema show tables; Your SQL will fail! The SQL doesn't quite work as expected! The reason is that each SQL statement executed in the SQL window has an independent context. What this means is that the catalog name is reset when you run another SQL statement after the USE statement is executed. If we bundle the two commands together, the SQL will work. Set the catalog to tpch.tiny and show the tables USE tpch.tiny; show tables; The describe command is used to display the format of a table. Inspect schema of the customer table use tpch.tiny; describe customer; You could also use the syntax below to achieve the same result. Show the customer table columns using an alternate format show columns from tpch.tiny.customer; The Presto engine includes a number of built-in functions. The following SQL will return a list of available Date functions. \u2003 Inspect available Date functions show functions like 'date%'; Now we will switch to a different schema and display the tables in it. In this example we use the show command with the IN clause to tell the system which catalog and schema to use. View the contents of the sf1 schema show tables in tpch.sf1; Query data from customer table select * from tpch.sf1.customer limit 5; Presto gathers statistics for tables in order to generate more accurate access plans. This SQL will gather statistics on a given table. Gather statistics on the customer table show stats for tpch.sf1.customer;","title":"Catalog"},{"location":"wxd-prestocli/#summary","text":"This lab has provided an introduction to some of the SQL that is available in Presto and watsonx.data. The next set of labs will demonstrate some more sophisticated SQL that can be run in the Presto engine.","title":"Summary"},{"location":"wxd-query/","text":"Query Workspace An earlier lab introduced the Query Workspace . In this section we will provide some best practices on how to use this workspace to run the SQL examples. Click on the Query Workspace icon on the left side of the screen The Query workspace will have a similar layout to the Data Manager screen, where you can navigate through the catalogs associated with the selected engine. The SQL work area is found to the right of the catalog and table names. The SQL work area will be used to run the commands in the lab. All the SQL examples will use the following format: Retrieve 10 rows from the ontime table SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is usually found on the far right-hand side of the command box. Use this to copy the text and paste it into the SQL window. You can also select the text and copy and paste it, although you need to be careful that you have selected all the text. You can also use the paste command in your operating system (Command-V for OSX and Control-V for Windows). Copy the following SQL and place it into the SQL window SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 20; Place your cursor at the front of the second statement and left click, then use the Drop Menu to reveal two run options There are four options for running statements: Run (no dropdown) - This will run all the statements in the SQL text found in the window Run to cursor - Run the statements up to the cursor position Run from cursor - Run the statement after the cursor position Run selection - Run the selected text Run to cursor The results displayed are for the first SQL statement (LIMIT 10). Run from cursor The results displayed are for the second SQL statement (LIMIT 20). Run (no dropdown) Both statements will execute. Scrolling down on the answer set will show that there are four result sets (one for each SQL statement executed). Run with selection (Select the first SQL line) The results should be the same as the first query. When running SQL statements you have the choice of opening up a new SQL window by pressing the [+] at the top of the SQL text box. There are some additional icons at the top of the SQL screen that you may find useful. These icons represent: Undo last change Redo last change Cut the selected text Copy the selected text Paste Comment out the line(s) lines selected Format the select SQL Format the Worksheet Clear the Worksheet Save the SQL Finally, there are two special characters that you should be aware of when dealing with SQL. The first is the statement terminator, the semicolon ; . The semicolon terminates an SQL statement. You can place multiple SQL statements into a SQL window and execute them as a block (Run) or by selecting the SQL text that you want to run and then pressing Run. The second special character is the double dash -- which is used to comment out everything after that position in the text. For instance, the following SQL will only run the second statement: -- SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 20; The comment characters become very useful when debugging SQL statements! Summary This lab you learned about some additional hints and tips on using the Query Workspace and the use of the SQL window.","title":"Using the Query Workspace"},{"location":"wxd-query/#query-workspace","text":"An earlier lab introduced the Query Workspace . In this section we will provide some best practices on how to use this workspace to run the SQL examples. Click on the Query Workspace icon on the left side of the screen The Query workspace will have a similar layout to the Data Manager screen, where you can navigate through the catalogs associated with the selected engine. The SQL work area is found to the right of the catalog and table names. The SQL work area will be used to run the commands in the lab. All the SQL examples will use the following format: Retrieve 10 rows from the ontime table SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is usually found on the far right-hand side of the command box. Use this to copy the text and paste it into the SQL window. You can also select the text and copy and paste it, although you need to be careful that you have selected all the text. You can also use the paste command in your operating system (Command-V for OSX and Control-V for Windows). Copy the following SQL and place it into the SQL window SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 20; Place your cursor at the front of the second statement and left click, then use the Drop Menu to reveal two run options There are four options for running statements: Run (no dropdown) - This will run all the statements in the SQL text found in the window Run to cursor - Run the statements up to the cursor position Run from cursor - Run the statement after the cursor position Run selection - Run the selected text Run to cursor The results displayed are for the first SQL statement (LIMIT 10). Run from cursor The results displayed are for the second SQL statement (LIMIT 20). Run (no dropdown) Both statements will execute. Scrolling down on the answer set will show that there are four result sets (one for each SQL statement executed). Run with selection (Select the first SQL line) The results should be the same as the first query. When running SQL statements you have the choice of opening up a new SQL window by pressing the [+] at the top of the SQL text box. There are some additional icons at the top of the SQL screen that you may find useful. These icons represent: Undo last change Redo last change Cut the selected text Copy the selected text Paste Comment out the line(s) lines selected Format the select SQL Format the Worksheet Clear the Worksheet Save the SQL Finally, there are two special characters that you should be aware of when dealing with SQL. The first is the statement terminator, the semicolon ; . The semicolon terminates an SQL statement. You can place multiple SQL statements into a SQL window and execute them as a block (Run) or by selecting the SQL text that you want to run and then pressing Run. The second special character is the double dash -- which is used to comment out everything after that position in the text. For instance, the following SQL will only run the second statement: -- SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 20; The comment characters become very useful when debugging SQL statements!","title":"Query Workspace"},{"location":"wxd-query/#summary","text":"This lab you learned about some additional hints and tips on using the Query Workspace and the use of the SQL window.","title":"Summary"},{"location":"wxd-queryhistory/","text":"Query History The Query History page displays the list of queries that were run while the Presto engine was running. It includes SQL that was run by a user along with SQL that was run by the watsonx.data UI to populate the values in the dialog. Click on the Query History icon on the left side of the screen The Query History dialog provides a list of all SQL statements sorted by creation time. The table contains the following columns: Query ID - The unique query ID assigned to the SQL statement Query - The SQL text that was executed State - What the statement is currently doing (Finished, Running) Engine - What engine the query was executed on User - The user that submitted the query Source - Where the query was initiated from Created - Creation date and time of the query There are two additional columns which are not displayed: Queued time - The amount of time the query waited to execute Analysis time - The amount of time it took to optimize the query The columns that are displayed can be adjusted by using the customize option at the top of the query list. Remove Engine and Source and Add Queued and Analysis Time You can further refine the list by clicking on individual columns to sort in ascending or descending order. Sort the results by Analysis time Additional filters are available by pressing the filter button above the query list. Filter by the Source being the System If you find a query that you want to see more details on, click on the kebab symbol at the end of the line \u22ee You have the choice of viewing the execution plan of the query, or placing the query into the Query workspace. This provides a simple of way of retrieving SQL that you may have deleted from your Query workspace! The execution plan may be useful to debug performance issues with a query. Clear the filters (found at the top of the query list) and sort by Query ID Your screens will be different that the ones found in the lab Find the first SQL statement in your list of queries and select the View Excecution Plan option The panel displayed will be similar to the following. Select the Query Statement tab The SQL statement that was executed is found here. It may be a system routine if you did not filter the queries by userid. Select the Logical Execution Details tab This panel provides the logical steps that the engine goes through to compute the answer set. Select the Distributed Execution Details This panel provides information how the query is distributed among engines. Select the Explain Analysis This panel provides details on the explain analysis (costs) of the various steps in executing the query. Query statement - The original SQL that was submitted to the engine Logical Execution - Distributed Execution Explain Analyze Summary In this lab you learned how to look at previous queries, examine the SQL that was executed, and view the explain plans that were generated by the Presto engine.","title":"Query History"},{"location":"wxd-queryhistory/#query-history","text":"The Query History page displays the list of queries that were run while the Presto engine was running. It includes SQL that was run by a user along with SQL that was run by the watsonx.data UI to populate the values in the dialog. Click on the Query History icon on the left side of the screen The Query History dialog provides a list of all SQL statements sorted by creation time. The table contains the following columns: Query ID - The unique query ID assigned to the SQL statement Query - The SQL text that was executed State - What the statement is currently doing (Finished, Running) Engine - What engine the query was executed on User - The user that submitted the query Source - Where the query was initiated from Created - Creation date and time of the query There are two additional columns which are not displayed: Queued time - The amount of time the query waited to execute Analysis time - The amount of time it took to optimize the query The columns that are displayed can be adjusted by using the customize option at the top of the query list. Remove Engine and Source and Add Queued and Analysis Time You can further refine the list by clicking on individual columns to sort in ascending or descending order. Sort the results by Analysis time Additional filters are available by pressing the filter button above the query list. Filter by the Source being the System If you find a query that you want to see more details on, click on the kebab symbol at the end of the line \u22ee You have the choice of viewing the execution plan of the query, or placing the query into the Query workspace. This provides a simple of way of retrieving SQL that you may have deleted from your Query workspace! The execution plan may be useful to debug performance issues with a query. Clear the filters (found at the top of the query list) and sort by Query ID Your screens will be different that the ones found in the lab Find the first SQL statement in your list of queries and select the View Excecution Plan option The panel displayed will be similar to the following. Select the Query Statement tab The SQL statement that was executed is found here. It may be a system routine if you did not filter the queries by userid. Select the Logical Execution Details tab This panel provides the logical steps that the engine goes through to compute the answer set. Select the Distributed Execution Details This panel provides information how the query is distributed among engines. Select the Explain Analysis This panel provides details on the explain analysis (costs) of the various steps in executing the query. Query statement - The original SQL that was submitted to the engine Logical Execution - Distributed Execution Explain Analyze","title":"Query History"},{"location":"wxd-queryhistory/#summary","text":"In this lab you learned how to look at previous queries, examine the SQL that was executed, and view the explain plans that were generated by the Presto engine.","title":"Summary"},{"location":"wxd-queryworkspace/","text":"Query Workspace Databases and query engines such as Presto have multiple ways that users can interact with the data. For example, there is usually an interactive command line interface (CLI) that lets users run SQL statements from a command terminal. Applications can use JDBC (Java Database Connectivity) to connect to the data store and run SQL statements. The watsonx.data user interface includes an SQL interface for building and running SQL statements. This is called the Query workspace. Users can write or copy in their own SQL statements, or they can use templates to assist in building new SQL statements. Click on the Query Workspace icon on the left side of the screen The Query workspace will have a similar layout to the Data Manager screen, where you can navigate through the catalogs associated with the selected engine. The SQL work area is found to the right of the catalog and table names. You can type, paste, or use saved workspaces to enter SQL (Structured Query Language) commands into this window. An SQL statement instructs the database engine what records to look for in one or more tables. The Query workspace provides several pre-built SQL statements that can be generated for tables already cataloged in the system. In the Data Manager lab, the Data sample tab was used to view a subset of the data in the airline_delay_cause table. This data can be generated in the Query Workspace window by using a prebuilt SQL generator. Expand the hive_data catalog by clicking on the \u25ba beside the hive_data name You should now see all the schemas associated with the catalog, including the ontime schema. Expand the ontime schema by pressing \u25ba beside the schema name The ontime schema includes 5 tables. Hover your mouse over the ontime table name When your mouse hovers over a table name, two icons will become visible. The first icon the refresh symbol \u21ba . The refresh icon forces the watsonx.data UI to retrieve details about the schema or table. Note that watsonx.data does not always reflect the status of tables in the display. The refresh process in not synchronous (i.e., it does not know that an update may have occurred to a table), so changes are not immediately shown in the display. The second icon </> is used to display query templates. Click on the query template icon when it appears on the ontime table line The menu provides four templates for your table: Generate Path \u2013 The path description (or location) of the object. Generate Select \u2013 An SQL statement that will return the contents of the table. Generate Alter \u2013 An SQL statement (DDL \u2013 Data Definition Language) used to alter characteristics of the table, like adding a column. Generate Drop \u2013 An SQL statement used to remove the object from the system. Select the Generate SELECT option The system has populated the SQL window with the catalog name, the schema and the table name. This SQL will retrieve all the columns of the table and limit the output to 10 rows (LIMIT 10). The Run on presto-01 button is highlighted at the far right of the SQL window. The button will reflect which engine watsonx.data will run your query on. There is a pulldown menu beside the button that provides an option to run the query to the point where your cursor is on the text, or from where your cursor is on the text. You can use these options to run portions of a SQL statement when you are debugging results. In this instance, you just want to run the query and display the results. Press the Run on presto-01 button and wait for the results to return Near the middle of the screen, you will see the statement that was executed, the run time, and the success or failure of the statement. The result set (if there is one) is displayed below the SQL statement. An export icon \u2913 is found on the far-right side of the result set. Clicking on the export icon will download the results as a CSV file on your workstation. Select the Details tab in the result set window This dialog provides more information on the execution of the SQL statement. If you want additional details on the query execution plan, you can press the Explain button beside the Run on presto-01 button. Press the Explain button in the SQL window The graph displays the steps the Presto engine took to compute the answer. You can click on any one of the boxes in the diagram to display the details of the step. The system will not show many details because the data is in a hive catalog and not optimized for retrieval. Close the Explain dialog by pressing the [x] If you find that you are running the same SQL repeatedly, you can save the SQL as a worksheet. The SQL window should still have your select statement that was generated to retrieve the first 10 rows of the table. The save button is found above the SQL. Press the save button The Save icon will open a dialog to save the SQL statement. Enter the name Quick Look and press the Save button At the bottom of your catalog and table list you will see a list of saved worksheets. In there should be the one you just saved called Quick Look . If you do not see your file, refresh your browser window Click on the Quick Look worksheet you created When you click on the name of a worksheet, the SQL will be placed into a new tab in the SQL window, with the tab containing the name of your saved worksheet. You can now modify the SQL before running it to retrieve results. Summary This chapter explored the use of the Query Workspace to run SQL commands, generate queries, and determine how the optimizer ran a query. In addition, the ability to store and retrieve was used.","title":"Query Workspace"},{"location":"wxd-queryworkspace/#query-workspace","text":"Databases and query engines such as Presto have multiple ways that users can interact with the data. For example, there is usually an interactive command line interface (CLI) that lets users run SQL statements from a command terminal. Applications can use JDBC (Java Database Connectivity) to connect to the data store and run SQL statements. The watsonx.data user interface includes an SQL interface for building and running SQL statements. This is called the Query workspace. Users can write or copy in their own SQL statements, or they can use templates to assist in building new SQL statements. Click on the Query Workspace icon on the left side of the screen The Query workspace will have a similar layout to the Data Manager screen, where you can navigate through the catalogs associated with the selected engine. The SQL work area is found to the right of the catalog and table names. You can type, paste, or use saved workspaces to enter SQL (Structured Query Language) commands into this window. An SQL statement instructs the database engine what records to look for in one or more tables. The Query workspace provides several pre-built SQL statements that can be generated for tables already cataloged in the system. In the Data Manager lab, the Data sample tab was used to view a subset of the data in the airline_delay_cause table. This data can be generated in the Query Workspace window by using a prebuilt SQL generator. Expand the hive_data catalog by clicking on the \u25ba beside the hive_data name You should now see all the schemas associated with the catalog, including the ontime schema. Expand the ontime schema by pressing \u25ba beside the schema name The ontime schema includes 5 tables. Hover your mouse over the ontime table name When your mouse hovers over a table name, two icons will become visible. The first icon the refresh symbol \u21ba . The refresh icon forces the watsonx.data UI to retrieve details about the schema or table. Note that watsonx.data does not always reflect the status of tables in the display. The refresh process in not synchronous (i.e., it does not know that an update may have occurred to a table), so changes are not immediately shown in the display. The second icon </> is used to display query templates. Click on the query template icon when it appears on the ontime table line The menu provides four templates for your table: Generate Path \u2013 The path description (or location) of the object. Generate Select \u2013 An SQL statement that will return the contents of the table. Generate Alter \u2013 An SQL statement (DDL \u2013 Data Definition Language) used to alter characteristics of the table, like adding a column. Generate Drop \u2013 An SQL statement used to remove the object from the system. Select the Generate SELECT option The system has populated the SQL window with the catalog name, the schema and the table name. This SQL will retrieve all the columns of the table and limit the output to 10 rows (LIMIT 10). The Run on presto-01 button is highlighted at the far right of the SQL window. The button will reflect which engine watsonx.data will run your query on. There is a pulldown menu beside the button that provides an option to run the query to the point where your cursor is on the text, or from where your cursor is on the text. You can use these options to run portions of a SQL statement when you are debugging results. In this instance, you just want to run the query and display the results. Press the Run on presto-01 button and wait for the results to return Near the middle of the screen, you will see the statement that was executed, the run time, and the success or failure of the statement. The result set (if there is one) is displayed below the SQL statement. An export icon \u2913 is found on the far-right side of the result set. Clicking on the export icon will download the results as a CSV file on your workstation. Select the Details tab in the result set window This dialog provides more information on the execution of the SQL statement. If you want additional details on the query execution plan, you can press the Explain button beside the Run on presto-01 button. Press the Explain button in the SQL window The graph displays the steps the Presto engine took to compute the answer. You can click on any one of the boxes in the diagram to display the details of the step. The system will not show many details because the data is in a hive catalog and not optimized for retrieval. Close the Explain dialog by pressing the [x] If you find that you are running the same SQL repeatedly, you can save the SQL as a worksheet. The SQL window should still have your select statement that was generated to retrieve the first 10 rows of the table. The save button is found above the SQL. Press the save button The Save icon will open a dialog to save the SQL statement. Enter the name Quick Look and press the Save button At the bottom of your catalog and table list you will see a list of saved worksheets. In there should be the one you just saved called Quick Look . If you do not see your file, refresh your browser window Click on the Quick Look worksheet you created When you click on the name of a worksheet, the SQL will be placed into a new tab in the SQL window, with the tab containing the name of your saved worksheet. You can now modify the SQL before running it to retrieve results.","title":"Query Workspace"},{"location":"wxd-queryworkspace/#summary","text":"This chapter explored the use of the Query Workspace to run SQL commands, generate queries, and determine how the optimizer ran a query. In addition, the ability to store and retrieve was used.","title":"Summary"},{"location":"wxd-reference-access/","text":"Accessing the watsonx.data TechZone Image The reservation email from TechZone is extremely important since it provides a link to your reservation. Click on the View My Reservations to access your reservations. Click on the reservation that corresponds to the watsonx.data reservation. The menu button that is beside the arrow provides options to extend or delete the reservation. When you click on reservation details option, or the reservation box, the browser will display the details of your image. Scroll down to the bottom of the web page to access the VM Remote Console. You can access the logon screen of the virtual machine by pressing the VM Remote Console button. It is not necessary to use the VM console except unless you want to use the dBeaver program. Select the watsonx user and use watsonx.data as the password. Refer to the section on VM Remote Console for more details.","title":"Accessing the watsonx.data TechZone Image"},{"location":"wxd-reference-access/#accessing-the-watsonxdata-techzone-image","text":"The reservation email from TechZone is extremely important since it provides a link to your reservation. Click on the View My Reservations to access your reservations. Click on the reservation that corresponds to the watsonx.data reservation. The menu button that is beside the arrow provides options to extend or delete the reservation. When you click on reservation details option, or the reservation box, the browser will display the details of your image. Scroll down to the bottom of the web page to access the VM Remote Console. You can access the logon screen of the virtual machine by pressing the VM Remote Console button. It is not necessary to use the VM console except unless you want to use the dBeaver program. Select the watsonx user and use watsonx.data as the password. Refer to the section on VM Remote Console for more details.","title":"Accessing the watsonx.data TechZone Image"},{"location":"wxd-reference-console/","text":"Using the VM Remote Console The watsonx server that has been provisioned has no physical monitor attached to it (headless is what it is commonly referred to) and so we need to use a different technique to view the desktop of the main user or the system (watsonx). The first thing to consider is whether you need to use the VM Remote Console at all. All the services like the watsonx.data UI, MinIO, Presto, Apache Superset and Portainer, are all web-based servers which means you just need to use your own browser to access these programs. Connecting into the watsonx virtual machine can be done using the secure shell command (ssh) which provides access to all the low-level commands you might need to use like starting the Apache Superset service. Note that Apache Superset is not up and running by default, so you will need to start it before attempting to connect to it. So what's the VM Remote Console required for? One program that has been provided to view the database schemas is dBeaver, a community edition of software that provides a query interface to 100's of data sources, including the watsonx.data environment. You can only use this program using the VM Remote Console. You do have the option of installing this software on your own machine if you wish. Find your email message that contains details of your reservation. Details of what the reservations and the page containing details of the reservation can be found in the Accessing the reservation or Accessing a workshop section. Once the details appear, scroll down to the bottom of the web page, and you will see the VM Remote Console button. You can access the logon screen of the virtual machine by pressing the VM Remote Console button. Clicking on this button will display the logon screen for the server. Select the watsonx user and use watsonx.data as the password. You can open this window in a separate browser window, or place it into full-screen mode. Note that you may need to increase the size of your browser window (or change the scaling in the browser) to see all the virtual desktop. At this point you have access to the desktop of the watsonx user and can issue commands from within this environment. As mentioned previously, you do not need to use this interface to use the lab.","title":"Using the VM Remote Console"},{"location":"wxd-reference-console/#using-the-vm-remote-console","text":"The watsonx server that has been provisioned has no physical monitor attached to it (headless is what it is commonly referred to) and so we need to use a different technique to view the desktop of the main user or the system (watsonx). The first thing to consider is whether you need to use the VM Remote Console at all. All the services like the watsonx.data UI, MinIO, Presto, Apache Superset and Portainer, are all web-based servers which means you just need to use your own browser to access these programs. Connecting into the watsonx virtual machine can be done using the secure shell command (ssh) which provides access to all the low-level commands you might need to use like starting the Apache Superset service. Note that Apache Superset is not up and running by default, so you will need to start it before attempting to connect to it. So what's the VM Remote Console required for? One program that has been provided to view the database schemas is dBeaver, a community edition of software that provides a query interface to 100's of data sources, including the watsonx.data environment. You can only use this program using the VM Remote Console. You do have the option of installing this software on your own machine if you wish. Find your email message that contains details of your reservation. Details of what the reservations and the page containing details of the reservation can be found in the Accessing the reservation or Accessing a workshop section. Once the details appear, scroll down to the bottom of the web page, and you will see the VM Remote Console button. You can access the logon screen of the virtual machine by pressing the VM Remote Console button. Clicking on this button will display the logon screen for the server. Select the watsonx user and use watsonx.data as the password. You can open this window in a separate browser window, or place it into full-screen mode. Note that you may need to increase the size of your browser window (or change the scaling in the browser) to see all the virtual desktop. At this point you have access to the desktop of the watsonx user and can issue commands from within this environment. As mentioned previously, you do not need to use this interface to use the lab.","title":"Using the VM Remote Console"},{"location":"wxd-reference-documentation/","text":"Documentation The following links provide more information on the components in this lab. watsonx.data - https://www.ibm.com/docs/en/watsonxdata/1.1.x Presto SQL - https://prestodb.io/docs/current/sql.html Presto Console - https://prestodb.io/docs/current/admin/web-interface.html MinIO - https://min.io/docs/minio/linux/administration/minio-console.html MinIO CLI - https://min.io/docs/minio/linux/reference/minio-mc.html Apache Superset - https://superset.apache.org/docs/creating-charts-dashboards/exploring-data dBeaver - https://dbeaver.com/docs/wiki/Application-Window-Overview/ Db2 SQL - https://www.ibm.com/docs/en/db2/11.5?topic=queries-select-statement PostgreSQL SQL - https://www.postgresql.org/docs/current/sql.html MySQL SQL - https://dev.mysql.com/doc/refman/8.1/en/sql-statements.html","title":"Documentation"},{"location":"wxd-reference-documentation/#documentation","text":"The following links provide more information on the components in this lab. watsonx.data - https://www.ibm.com/docs/en/watsonxdata/1.1.x Presto SQL - https://prestodb.io/docs/current/sql.html Presto Console - https://prestodb.io/docs/current/admin/web-interface.html MinIO - https://min.io/docs/minio/linux/administration/minio-console.html MinIO CLI - https://min.io/docs/minio/linux/reference/minio-mc.html Apache Superset - https://superset.apache.org/docs/creating-charts-dashboards/exploring-data dBeaver - https://dbeaver.com/docs/wiki/Application-Window-Overview/ Db2 SQL - https://www.ibm.com/docs/en/db2/11.5?topic=queries-select-statement PostgreSQL SQL - https://www.postgresql.org/docs/current/sql.html MySQL SQL - https://dev.mysql.com/doc/refman/8.1/en/sql-statements.html","title":"Documentation"},{"location":"wxd-reference-ibmid/","text":"Requesting an IBM Userid You must obtain an IBM Login Userid to use the lab An IBMid is needed to access IBM Technology Zone. If you do not have an IBMid, click on the following link. https://techzone.ibm.com You should see the following login screen for TechZone. Click on the `Create an IBMid`` button and proceed to fill in the details on this form: Once you have verified your account, you can continue onto logging into the TechZone server.","title":"Requesting an IBMid"},{"location":"wxd-reference-ibmid/#requesting-an-ibm-userid","text":"You must obtain an IBM Login Userid to use the lab An IBMid is needed to access IBM Technology Zone. If you do not have an IBMid, click on the following link. https://techzone.ibm.com You should see the following login screen for TechZone. Click on the `Create an IBMid`` button and proceed to fill in the details on this form: Once you have verified your account, you can continue onto logging into the TechZone server.","title":"Requesting an IBM Userid"},{"location":"wxd-reference-passwords/","text":"Passwords This table lists the passwords for the services that have \"fixed\" userids and passwords. Service Userid Password Virtual Machine watsonx watsonx.data Virtual Machine root watsonx.data watsonx.data UI ibmlhadmin password Jupyter Notebook none watsonx.data Presto ibmlhadmin password Minio Generated Generated Postgres admin Generated Apache Superset admin admin Portainer admin watsonx.data Db2 db2inst1 db2inst1 MySQL root password VNC Windows none watsonx. VNC OSX none watsonx.data Use the following commands to get the generated userid and password for MinIO. export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') echo \"MinIO Userid : \" $LH_S3_ACCESS_KEY echo \"MinIO Password: \" $LH_S3_SECRET_KEY Use the following command to get the password for Postgres. export POSTGRES_PASSWORD=$(docker exec ibm-lh-postgres printenv | grep POSTGRES_PASSWORD | sed 's/.*=//') echo \"Postgres Userid : admin\" echo \"Postgres Password : \" $POSTGRES_PASSWORD You can get all passwords for the system when you are logged by issuing the following command: cat /certs/passwords If the passwords do not appear to work, you may need to regenerate them. The following must be run as the root user. sudo su - passwords The passwords command will refresh the passwords and also display them. If this command is not run as root, an error message will be displayed because the password file cannot be updated as the watsonx user.","title":"Passwords"},{"location":"wxd-reference-passwords/#passwords","text":"This table lists the passwords for the services that have \"fixed\" userids and passwords. Service Userid Password Virtual Machine watsonx watsonx.data Virtual Machine root watsonx.data watsonx.data UI ibmlhadmin password Jupyter Notebook none watsonx.data Presto ibmlhadmin password Minio Generated Generated Postgres admin Generated Apache Superset admin admin Portainer admin watsonx.data Db2 db2inst1 db2inst1 MySQL root password VNC Windows none watsonx. VNC OSX none watsonx.data Use the following commands to get the generated userid and password for MinIO. export LH_S3_ACCESS_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed 's/.*=//') export LH_S3_SECRET_KEY=$(docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//') echo \"MinIO Userid : \" $LH_S3_ACCESS_KEY echo \"MinIO Password: \" $LH_S3_SECRET_KEY Use the following command to get the password for Postgres. export POSTGRES_PASSWORD=$(docker exec ibm-lh-postgres printenv | grep POSTGRES_PASSWORD | sed 's/.*=//') echo \"Postgres Userid : admin\" echo \"Postgres Password : \" $POSTGRES_PASSWORD You can get all passwords for the system when you are logged by issuing the following command: cat /certs/passwords If the passwords do not appear to work, you may need to regenerate them. The following must be run as the root user. sudo su - passwords The passwords command will refresh the passwords and also display them. If this command is not run as root, an error message will be displayed because the password file cannot be updated as the watsonx user.","title":"Passwords"},{"location":"wxd-reference-techzone/","text":"Requesting a TechZone image If you are part of a workshop, you do not have to request a reservation. Instead, go to the Accessing a Workshop section. Log into TechZone ( https://techzone.ibm.com ) and search for the watsonx.data Developer Base Image or use the following link. https://techzone.ibm.com/collection/ibm-watsonxdata-developer-base-image If you have not logged into the IBM Cloud site, you will be asked to authenticate with your IBM userid. If you do not have an IBM userid, you will need to register for one. This lab is open to IBMers and Business Partners. Once you have logged in, you should see the following. Select the Environment tab on the far-left side. Note : There may be more than one environment available. Choose the one best suited for your requirements. Press the Reserve button. Select \"reserve now\" (why wait?). For \"Purpose\" select Self Education. This will expand to request additional information. Fill in the purpose field with something meaningful (watsonx.data education). Next select preferred Geography for the image. Choose any of the regions that are closest to your location. Note : The TechZone scheduler will pick a location in your region that has capacity to deploy your image. Previously you needed to pick a physical location (DAL10, WDC04, TOK02, etc...). The number of locations has expanded to 4 North American, 4 European and 2 AP locations which will hopefully provide more capacity to deploy the lab. If you find that your reservation is not being provisioned, check the status of the TechZone environment by referring to the TechZone status page at https://techzone.status.io . Next select the end date for the lab. Make sure you select enough time for you to use the lab! It defaults to 2 days, but you can extend the reservation! You do not need to enable VPN Access . Once you have completed the form, check the box indicating that you agree to the terms and conditions of using TechZone, and click SUBMIT on the bottom right-hand corner. At this point you will need to wait patiently for an email that acknowledges that your request has been placed into Provisioning mode. Eventually you will receive an email confirming that the system is ready to be used. Note that this can take a number of hours depending on the load on the TechZone servers. You may also get a message telling you that the system provisioning has Failed. Ignore the reason field since it is usually related to an environment failure caused by lack of resources. Check the status of TechZone first ( https://techzone.status.io ). If the systems appear to be okay, try requesting another image or using a different server location if possible. Contact TechZone support if you are having difficulties provisioning a system.","title":"Requesting a TechZone image"},{"location":"wxd-reference-techzone/#requesting-a-techzone-image","text":"If you are part of a workshop, you do not have to request a reservation. Instead, go to the Accessing a Workshop section. Log into TechZone ( https://techzone.ibm.com ) and search for the watsonx.data Developer Base Image or use the following link. https://techzone.ibm.com/collection/ibm-watsonxdata-developer-base-image If you have not logged into the IBM Cloud site, you will be asked to authenticate with your IBM userid. If you do not have an IBM userid, you will need to register for one. This lab is open to IBMers and Business Partners. Once you have logged in, you should see the following. Select the Environment tab on the far-left side. Note : There may be more than one environment available. Choose the one best suited for your requirements. Press the Reserve button. Select \"reserve now\" (why wait?). For \"Purpose\" select Self Education. This will expand to request additional information. Fill in the purpose field with something meaningful (watsonx.data education). Next select preferred Geography for the image. Choose any of the regions that are closest to your location. Note : The TechZone scheduler will pick a location in your region that has capacity to deploy your image. Previously you needed to pick a physical location (DAL10, WDC04, TOK02, etc...). The number of locations has expanded to 4 North American, 4 European and 2 AP locations which will hopefully provide more capacity to deploy the lab. If you find that your reservation is not being provisioned, check the status of the TechZone environment by referring to the TechZone status page at https://techzone.status.io . Next select the end date for the lab. Make sure you select enough time for you to use the lab! It defaults to 2 days, but you can extend the reservation! You do not need to enable VPN Access . Once you have completed the form, check the box indicating that you agree to the terms and conditions of using TechZone, and click SUBMIT on the bottom right-hand corner. At this point you will need to wait patiently for an email that acknowledges that your request has been placed into Provisioning mode. Eventually you will receive an email confirming that the system is ready to be used. Note that this can take a number of hours depending on the load on the TechZone servers. You may also get a message telling you that the system provisioning has Failed. Ignore the reason field since it is usually related to an environment failure caused by lack of resources. Check the status of TechZone first ( https://techzone.status.io ). If the systems appear to be okay, try requesting another image or using a different server location if possible. Contact TechZone support if you are having difficulties provisioning a system.","title":"Requesting a TechZone image"},{"location":"wxd-reference-workshop/","text":"Accessing a Workshop To access a watsonx.data workshop, you will need to have an IBM userid and a link provided to your class instructor. This link will first ask you to log into the system using your IBM userid, and then a screen similar to the following will be displayed: The title of the workshop may be different, but the steps to access the lab will remain the same. The class instructor will have provided a unique password for the course. Enter that value into the password/access code box and click on the Submit button. When the connection is successful, the details of your environment will be shown in the browser. The top of the page contains all the published services that you will use during the lab. For instance, if the lab requires that you access the Presto console, you would click on the link in this browser which says: Presto console - https://useast.services.cloud.techzone.ibm.com:xxxxx Watsonx.data URLs Each user will have a unique set of URLs for their watsonx.data environment. Your URL service name and port number will be different than the examples shown in this lab. Make sure to use your URLs in any examples that require a server URL address and port number.","title":"Accessing a workshop"},{"location":"wxd-reference-workshop/#accessing-a-workshop","text":"To access a watsonx.data workshop, you will need to have an IBM userid and a link provided to your class instructor. This link will first ask you to log into the system using your IBM userid, and then a screen similar to the following will be displayed: The title of the workshop may be different, but the steps to access the lab will remain the same. The class instructor will have provided a unique password for the course. Enter that value into the password/access code box and click on the Submit button. When the connection is successful, the details of your environment will be shown in the browser. The top of the page contains all the published services that you will use during the lab. For instance, if the lab requires that you access the Presto console, you would click on the link in this browser which says: Presto console - https://useast.services.cloud.techzone.ibm.com:xxxxx Watsonx.data URLs Each user will have a unique set of URLs for their watsonx.data environment. Your URL service name and port number will be different than the examples shown in this lab. Make sure to use your URLs in any examples that require a server URL address and port number.","title":"Accessing a Workshop"},{"location":"wxd-timetravel/","text":"Time Travel Time travel allows you change the view of the data to a previous time. This is not the same as an AS OF query commonly used in SQL. The data is rolled back to a prior time. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Let us look at the snapshots available for the customer table in the workshop schema. Check current snapshots \u2013 STARTING STATE SELECT * FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; You need to capture the first snapshot ID returned by the SQL statement. You will need this value when you run the rollback command. Capture the first snapshot ID SELECT snapshot_id FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; Remember the number that was returned with the query above. Your snapshot ID will be different than the examples in this lab Insert the following record to change the customer table in the workshop schema. Insert a new customer record into the table insert into iceberg_data.workshop.customer values(1501,'Deepak','IBM SVL',16,'123-212-3455', 123,'AUTOMOBILE','Testing snapshots'); Let's look at the snapshots available for the customer table in the workshop schema. You should have 2 snapshots. View available snapshots SELECT * FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; Querying the customer table in the workshop schema, we can see the record inserted with name=\u2019Deepak\u2019. Select customer Deepak select * from iceberg_data.workshop.customer where name='Deepak'; We realize that we don't want the recent updates or just want to see what the data was at any point in time to respond to regulatory requirements. We will leverage the out-of-box system function rollback_to_snapshot to rollback to an older snapshot. The syntax for this function is: CALL iceberg_data.system.rollback_to_snapshot('workshop','customer',x); The \"x\" would get replaced with the snapshot_id number that was found in the earlier query. It will be different on your system than the examples above. View the available snapshots SELECT snapshot_id FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; Copy the first snapshot ID back into the SQL window by itself. Then when you copy the next command, make sure to paste the SQL before the number, and then you can edit the SQL to add the snapshot ID. Rollback to previous snapshot - Replace the ID parameter with the snapshot ID previously found. Remember the closing parenthesis! CALL iceberg_data.system.rollback_to_snapshot('workshop','customer',ID) The only output that is produced is a true or false value. Querying the customer table in the workshop schema, we cannot see the record inserted with name=\u2019Deepak\u2019. Select customer Deepak select * from iceberg_data.workshop.customer where name='Deepak'; Summary In this lab you learned how to rollback transactions to a previous checkpoint in the table.","title":"Time Travel"},{"location":"wxd-timetravel/#time-travel","text":"Time travel allows you change the view of the data to a previous time. This is not the same as an AS OF query commonly used in SQL. The data is rolled back to a prior time. Before starting, make sure you are in the Query Workspace by clicking this icon on the left side Let us look at the snapshots available for the customer table in the workshop schema. Check current snapshots \u2013 STARTING STATE SELECT * FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; You need to capture the first snapshot ID returned by the SQL statement. You will need this value when you run the rollback command. Capture the first snapshot ID SELECT snapshot_id FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; Remember the number that was returned with the query above. Your snapshot ID will be different than the examples in this lab Insert the following record to change the customer table in the workshop schema. Insert a new customer record into the table insert into iceberg_data.workshop.customer values(1501,'Deepak','IBM SVL',16,'123-212-3455', 123,'AUTOMOBILE','Testing snapshots'); Let's look at the snapshots available for the customer table in the workshop schema. You should have 2 snapshots. View available snapshots SELECT * FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; Querying the customer table in the workshop schema, we can see the record inserted with name=\u2019Deepak\u2019. Select customer Deepak select * from iceberg_data.workshop.customer where name='Deepak'; We realize that we don't want the recent updates or just want to see what the data was at any point in time to respond to regulatory requirements. We will leverage the out-of-box system function rollback_to_snapshot to rollback to an older snapshot. The syntax for this function is: CALL iceberg_data.system.rollback_to_snapshot('workshop','customer',x); The \"x\" would get replaced with the snapshot_id number that was found in the earlier query. It will be different on your system than the examples above. View the available snapshots SELECT snapshot_id FROM iceberg_data.workshop.\"customer$snapshots\" ORDER BY committed_at; Copy the first snapshot ID back into the SQL window by itself. Then when you copy the next command, make sure to paste the SQL before the number, and then you can edit the SQL to add the snapshot ID. Rollback to previous snapshot - Replace the ID parameter with the snapshot ID previously found. Remember the closing parenthesis! CALL iceberg_data.system.rollback_to_snapshot('workshop','customer',ID) The only output that is produced is a true or false value. Querying the customer table in the workshop schema, we cannot see the record inserted with name=\u2019Deepak\u2019. Select customer Deepak select * from iceberg_data.workshop.customer where name='Deepak';","title":"Time Travel"},{"location":"wxd-timetravel/#summary","text":"In this lab you learned how to rollback transactions to a previous checkpoint in the table.","title":"Summary"},{"location":"wxd-watsonui/","text":"Watsonx.data Console UI Overview Your TechZone reservation will include the server name and port number to use when connecting to the watsonx.data UI. Watsonx UI - https://useast.services.cloud.techzone.ibm.com:xxxxx Click on the watsonx.data UI supplied link or copy the URL and port number into your browser You will get a Certificate error in Firefox: Select Advanced Choose 'Accept the Risk and Continue' If you are using Google Chrome, you can bypass the error message by typing in \"thisisunsafe\" or clicking on the \"Proceed to server name (unsafe)\" link. The server name value will be replaced with the name of the TechZone server you are connecting to. The watsonx.data UI will display with a login prompt. Enter the credentials for the ibmlhadmin user and press Log in Username: ibmlhadmin Password: password After entering the userid and password, the main watsonx.data UI will be displayed. Watsonx.data UI Navigation The main screen provides a snapshot of the objects that are currently found in the watsonx.data system. The infrastructure display shows that there is 1 engine, 3 catalogs and 3 buckets associated with the system. You can examine these objects by using the menu system found on the left side of the screen. Click on the hamburger icon This will provide a list of items that you can explore in the UI. You can also access this list by clicking on one of the following icons. A brief description of the items is found below. Display the Home page. Infrastructure Manager \u2013 Displays the current engines, buckets and databases associated with the installation. Data Manager \u2013 Used to explore the various data sources that are cataloged in the system. You can explore the schemas, tables, table layout and view a subset of the data with this option. Query Workspace \u2013 An SQL-based query tool for accessing the data. Query History \u2013 A list of SQL queries that were previously run across all engines. Access Control \u2013 Control who can access the data. Home Page Displays a summary of the watsonx.data system and provides a summary of recent activity in the system. Infrastructure manager The Infrastructure manager displays the current engines, buckets and databases associated with the installation. Click on the Query Workplace icon Data Manager The Data Manager is used to explore the various data sources that are cataloged in the system. You can explore the schemas, tables, table layout and view a subset of the data with this option. The display make take a few minutes to show the schemas in the system as it is querying the catalog and populating the descriptions on the screen. Click on the Data Manager icon Query Workplace The Query Workspace provides an SQL-based query tool for accessing the data. Click on the Query Workplace icon Query History The Query History displays a list of SQL queries that were previously run across all engines. Click on the Query History icon Access Control The Access Control dialog provides controls for who can access the data. Click on the Access Control icon Access Control Restrictions The Developer Edition does not provide the ability to add or remove users from the Access Control panel. The full version of watsonx.data does provide this capability. Add and removing users can be done through a command line interface instead. Summary In this lab you learned how to display the watsonx.data UI. You then explored some of the menu items that are found in the watsonx.data UI itself. The subsequent labs will explore the following menu items: Infrastructure Manager Data Manager Query Workspace The next section will examine the Infrastructure Manager and how it provides a graphical view of the watsonx.data system.","title":"Watsonx UI Overview"},{"location":"wxd-watsonui/#watsonxdata-console-ui-overview","text":"Your TechZone reservation will include the server name and port number to use when connecting to the watsonx.data UI. Watsonx UI - https://useast.services.cloud.techzone.ibm.com:xxxxx Click on the watsonx.data UI supplied link or copy the URL and port number into your browser You will get a Certificate error in Firefox: Select Advanced Choose 'Accept the Risk and Continue' If you are using Google Chrome, you can bypass the error message by typing in \"thisisunsafe\" or clicking on the \"Proceed to server name (unsafe)\" link. The server name value will be replaced with the name of the TechZone server you are connecting to. The watsonx.data UI will display with a login prompt. Enter the credentials for the ibmlhadmin user and press Log in Username: ibmlhadmin Password: password After entering the userid and password, the main watsonx.data UI will be displayed.","title":"Watsonx.data Console UI Overview"},{"location":"wxd-watsonui/#watsonxdata-ui-navigation","text":"The main screen provides a snapshot of the objects that are currently found in the watsonx.data system. The infrastructure display shows that there is 1 engine, 3 catalogs and 3 buckets associated with the system. You can examine these objects by using the menu system found on the left side of the screen. Click on the hamburger icon This will provide a list of items that you can explore in the UI. You can also access this list by clicking on one of the following icons. A brief description of the items is found below. Display the Home page. Infrastructure Manager \u2013 Displays the current engines, buckets and databases associated with the installation. Data Manager \u2013 Used to explore the various data sources that are cataloged in the system. You can explore the schemas, tables, table layout and view a subset of the data with this option. Query Workspace \u2013 An SQL-based query tool for accessing the data. Query History \u2013 A list of SQL queries that were previously run across all engines. Access Control \u2013 Control who can access the data.","title":"Watsonx.data UI Navigation"},{"location":"wxd-watsonui/#home-page","text":"Displays a summary of the watsonx.data system and provides a summary of recent activity in the system.","title":"Home Page"},{"location":"wxd-watsonui/#infrastructure-manager","text":"The Infrastructure manager displays the current engines, buckets and databases associated with the installation. Click on the Query Workplace icon","title":"Infrastructure manager"},{"location":"wxd-watsonui/#data-manager","text":"The Data Manager is used to explore the various data sources that are cataloged in the system. You can explore the schemas, tables, table layout and view a subset of the data with this option. The display make take a few minutes to show the schemas in the system as it is querying the catalog and populating the descriptions on the screen. Click on the Data Manager icon","title":"Data Manager"},{"location":"wxd-watsonui/#query-workplace","text":"The Query Workspace provides an SQL-based query tool for accessing the data. Click on the Query Workplace icon","title":"Query Workplace"},{"location":"wxd-watsonui/#query-history","text":"The Query History displays a list of SQL queries that were previously run across all engines. Click on the Query History icon","title":"Query History"},{"location":"wxd-watsonui/#access-control","text":"The Access Control dialog provides controls for who can access the data. Click on the Access Control icon Access Control Restrictions The Developer Edition does not provide the ability to add or remove users from the Access Control panel. The full version of watsonx.data does provide this capability. Add and removing users can be done through a command line interface instead.","title":"Access Control"},{"location":"wxd-watsonui/#summary","text":"In this lab you learned how to display the watsonx.data UI. You then explored some of the menu items that are found in the watsonx.data UI itself. The subsequent labs will explore the following menu items: Infrastructure Manager Data Manager Query Workspace The next section will examine the Infrastructure Manager and how it provides a graphical view of the watsonx.data system.","title":"Summary"}]}